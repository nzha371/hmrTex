% !TEX TS-program = make

% --------------------------------------------------------------------------------------
%                   LATEX TEMPLATE FOR DISSERTATION (HONS)
% --------------------------------------------------------------------------------------
\documentclass[11pt]{book}

\usepackage{amsfonts, amsmath, amssymb}  
\usepackage{times}
\usepackage[utf8]{inputenc}

%\usepackage[backref=page,pagebackref=true,linkcolor = blue,citecolor = red]{hyperref}
%\usepackage[backref=page]{backref}

\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\setlength{\oddsidemargin}{1.5cm}
\setlength{\evensidemargin}{0cm}
\setlength{\topmargin}{1mm}
\setlength{\headheight}{1.36cm}
\setlength{\headsep}{1.00cm}
%\setlength{\textheight}{20.84cm}
\setlength{\textheight}{19cm}
\setlength{\textwidth}{14.5cm}
\setlength{\marginparsep}{1mm}
\setlength{\marginparwidth}{3cm}
\setlength{\footskip}{2.36cm}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textsf{ \bf #1}}
\newcommand{\R}{\textsf{R}}



\begin{document}
\pagestyle{empty}

%: ----------------------------------------------------------------------
%:                  TITLE PAGE: name, degree,..
% ----------------------------------------------------------------------

\begin{center}

\vspace{1cm}

%%% Type the thesis title below%%%%%%%%%%%%%%%%
{\Huge         High-Performance Hadoop Map/Reduce R Interface Enhancement}

\vspace{25mm} 

\includegraphics[width=3.5cm]{logo}

 \vspace{35mm}

%%%%%Type Your Name Below%%%%%%%%%%%%
{\Large       Noah Zhang}

	\vspace{1ex}

Department of Statistics

The University of Auckland

	\vspace{5ex}

 %%%%%Typing Your Supervisors Name Below%%%%%%%%%%%%
Supervisor:             Simon Urbanek

	\vspace{30mm}

A dissertation submitted in partial fulfillment of the requirements for the degree of BSc(Hons) in Statistics, The University of Auckland, 2020.

\end{center}

\newpage


%: --------------------------------------------------------------
%:                  FRONT MATTER:  abstract,..
% --------------------------------------------------------------
\chapter*{Abstract}       
\setcounter{page}{1}
\pagestyle{headings}
% \pagenumbering{roman}

\addcontentsline{toc}{chapter}{Abstract}


 Put your abstract  here.  The abstract should contain a brief summary of the aim, methodologies, 
finding and conclusions of the dissertation.  The abstract should normally be fewer than 350 words.


One of the most widely used parallel programming models today is MapReduce. MapReduce is easy both to learn and use, and is especially useful in analyzing large datasets. While it is not suitable for several classes of scientific computing operations that are better served by message-passing interface or OpenMP, such as numerical linear algebra or finite element and finite difference computations, MapReduce's utility in workflows frequently called “big data” has made it a mainstay in high performance computing. This chapter introduces the MapReduce programming model and the Hadoop open-source framework which supports it.

Big data is concern massive amount, complex, growing data set from multiple autonomous sources. It has to deal with large
and complex dataset that can be structured, semi-structured or unstructured and will typically not fit into memory to be processed.
MapReduce is a programming model for processing large datasets distributed on a large clusters.A rapid growth of data in recent time,
Industries and academia required an intelligent data analysis tool that would be helpful to satisfy the need to analysis a large amount of
data. MapReduce framework is basically designed to compute data demanding applications to support effective decision making. Since
its introduction, remarkable research efforts have been put to make it more familiar to the users subsequently utilized to support the
execution of enormous data intensive applications. This survey paper highlights and investigates various applications using recent
MapReduce models.





%: --------------------------------------------------------------
%:                  END:  abstract
% --------------------------------------------------------------


%: ----------------------- contents ------------------------
\setcounter{secnumdepth}{3} % organisational level that receives a numbers
\setcounter{tocdepth}{3}    % print table of contents for level 3
\tableofcontents            % print the table of contents
% levels are: 0 - chapter, 1 - section, 2 - subsection, 3 - subsection

%: --------------------------------------------------------------
%:                  MAIN DOCUMENT SECTION
% --------------------------------------------------------------
	
\chapter{Introduction}%    \chapter{}  = level 1, top level

A thesis should always have an introduction.  The purpose is to describe the general subject area, state the research problem of interest, outline the main results of the thesis, and put the results in context with the wider subject area and its applications.

The main body of the text must be divided into a logical scheme which  is followed consistently throughout the work.  
 It usually starts with an introduction chapter  and ends with  a conclusion chapter. See, for example, the table of contents on page 3. 

There is strict  35-page limit  for an applied mathematics dissertation,  including  the references  but excluding  appendices. 

\chapter{Concept of Big Data}

Big data concerns huge amounts, complex and diverse sets of information that is growing at ever-increasing rates. The size of the data suggest that we will need to deviate from the traditional processing methodologies and to adapt an inexpensive and efficient way through distributed/parallel computing.\\

Today, we can find upwards of 800 million webpages providing documentation on big data. Enthusiasts believe that Big Data is the next big thing after Cloud [1]. 

\section{Characteristics of Big Data}

The concept of big data is generally vague without a formal definition, however the general consensus is that there are specific attributes that define big data The four characteristics of big data are Volume, Velocity, Variety and Veracity [2]. 

\begin{center}
\includegraphics[width=10cm]{4vbd}\\
\end{center}

The main characteristic that makes data “big” is its sheer volume. According to estimates by IBM, we can expect at least 40 Zettabytes (43 Trillion Gigabytes) of data to be created in 2020 [3]. The volume of data sets being processed and analysed has reached sizes larger than terabytes and even petabytes. This suggests that data sets these days are becoming too large to process within a single desktop machine/processor. \\

Velocity is the speed at which data is generated. High velocity data is generated with such a pace that it may require certain distributed processing techniques. Good examples of high velocity data includes social media posts.\\

Variety is the source of the data which can be found in different forms such as text, numerical, images, audio and video records. The variety in the data will require distinct processing capabilities or algorithms to handle different formats. \\

Veracity is the quality of the data. Information may be volatile or incomplete seen in low veracity data sets containing a high percentage of meaningless data referred to as noise. On the other hand, high veracity data hold records that are valuable to analyse and contribute in a meaningful way to the overall results.

\section{Structure of Big Data} 

Big data can be categorised as unstructured or structured.\\
 
Unstructured data has its internal structure but it is not structured through pre-defined data models or schema. As it may comes in many different formats, it cannot be stored in relational databases which also becomes a real challenge for systems to process and analyse. The unstructured data may be stored within non-relational databases like NoSQL.

Structured data, in contrast, is usually stored and managed in relational databases with predefined data models. Examples of relational database applications with structured data include customer information, sales transactions, airline reservations systems, and billing systems. This type of structured data within relational databases can be accessed using Structured Query language (SQL).\\
 
 \section{Challenges around Big Data}
 
 \subsection{Converting Unstructured Data}
 
The challenge today is that bulk of the data we have and store do not come in a nice structure that we can easily access and perform analysis on. In managing unstructured data, many organisation simply collect and store significant volumes of unstructured information without having the ability to process and interpret it. This is due to the difficulty and cost of exploring it. However, it is not to say there is no way of processing unstructured data. There is simply not a standard, or one conventional method of processing such data. There are ways that structured data can be created from unstructured data through various processing methods. By converting to structured data, the data can be universally understood by the ordinary user, easily digestible by data programs as well as having the ability to transfer to other data tools. Hence there is the motivation to turn this unstructured data into structured data so we can be more efficient in terms of processing and analysis. 

When converting from unstructured data to structured data, the questions we often ask is what data do we wish to extract, is it a word count index or is it a particular feature, topic or sentiment? Then we want to explore what the structure look like. Is it key value pairs, JSON, XML, or Tabular? In the processing, we analyse the data and retrieve a structured metadata that augments the original data source. The alternative is to extract and clean the data if we already know what features we wish to extract. 

\subsubsection{Example}

%% give example converting unstructured to structured

\subsection{Other Challenges}

Apart from the challenge to create structure from unstructured data, we are also concerned with the ability to process large concentration of data. Of course you may say that technology is improving at the same time, however, to have the biggest and strongest machine is not always ideal and cost-effective. Below are some alternative approaches that we can take to tackle the challenge around processing big data.

\subsubsection{Parallel Computing}

Parallel computing is to maximise the use of processors to perform multiple tasks assigned to them simultaneously. Memory in parallel systems can be either shared or distributed. The main use of parallel computing is to execute and process data efficiently since it saves time, allowing the execution of applications in a shorter wall-clock time. However, this is limited to the computing power of the single machine.

\subsubsection{Distributed Computing}

Distributed computing involves multiple computers, physical servers, virtual machines, containers or any other node that communicate and coordinate actions in order to appear as a single coherent system. This system is not able to use a shared memory due to each node being physically separated, so the computers can only exchange messages and data over a network. However, distributed computing makes all nodes in the cluster work together as if they were one. This  significantly improves the performance through parallelism in which each node simultaneously handle a subset of the overall task as well as reducing cost of running a single high performance machine.

The ideal solution is to have a distributed system with the capabilities of parallel processing. 

\chapter{MapReduce Framework}

At present, with data being generated at an exponential rate, there is the need to deploy data intensive application and storage clusters in order to keep up with the amount of data. To handle such problem, Google developed the MapReduce programming model for distributed computing based on the Java language [5].

\section{Concept of MapReduce}

There are multiple approaches for processing relatively smaller datasets, however larger datasets require a different approach specifically for data that is too big to fit in memory. The traditional approach to process data on a single machine is to break data into individual chunks which is then loaded and processed sequentially on the machine. 

MapReduce is a processing technique/programming model for distributed computing based on Java. It is designed to process large datasets using the same splitting approach by breaking down the dataset into independent chunks. But instead of processing the chunks sequentially, the chunks are processed in parallel on a collective group of computers known as a cluster. The results of the individual chunks are then aggregated and returned as output.\\

\includegraphics[width=8cm]{keyvalue} \\

The algorithm operates on key-value pairs, which means that it takes a set of input key-value pairs and produces a set of output key-value pairs. In addition, the user is required to specify two functions for the Mapper and Reducer. The Mapper takes the input and produce the intermediate key-value pairs which is passed to the Reducer to create a set of output key-value pairs. The Reducer combines all the values associated with each key to create a smaller set of values. This enables us to input data that may be too large to fit in standard memory.\\

In the simplest form of the MapReduce model, the user only specifies the Map function. In this case, the output will consist of the intermediate key-value pairs which is ordered by the keys. 

{\bf TEST: 
See also Section~\ref{sec:mapreduce}. }

\section{The MapReduce Process}
\label{sec:mapreduce}

The complete process typically consists of four operations namely, splitting, mapping, shuffling and reducing.\\

1. Splitting - the process whereby larger data sets are split into smaller data sets based on the block size. The default size of each block is 64MB, however this can be adjusted by the user. The number of mappers will depend on the number of blocks generated by the split, i.e. if there are 3 blocks then there will only be 3 mappers.\\

2. Mapping - the purpose of the mapper is to process the input data. In this phase, data in each chunk is passed to a user specified map function and returns output in the form of key-value pairs. For example, if a file contains 100 blocks to be processed, you can have 100 mappers which can run together to process one block each or have 50 mappers running together to process two blocks each. The Hadoop framework decides how many mappers to allocate based on the size of the data and the memory block available on each mapper server.\\

3. Shuffling - the process by which data from the mappers is transferred to the reducers known as the intermediate values. The data from all mappers are sorted by the key, split among the reducers and sorted by the key. Without this step, there will be no input for the reducer.

4. Reducing - the reducer is guaranteed to have all records associated with each key. In this phase, the key-value output from the shuffling stage are aggregated.  It is important to note that one key can only be in one reducer, else the aggregation will be non-functional. In other words, this operation is a summary of the complete dataset. 

obtains the intermediate values, sorted by the key. The value list contains all values with the same key produced by mappers. Each reducer emits zero, one or multiple output key/value pairs for each input key/value pair.


\section{Examples}



\section{Advantages of MapReduce}

The main advantage of MapReduce is that it is highly scalable. This is due to the ability to process large data sets across multiple computer nodes. These servers can be inexpensive and can also operate in parallel. This simple scalability is what attracts many users to utilise the MapReduce model.

Other advantages include cost efficiency, security and authentication, parallel processing, availability, resilient and simple.


\chapter{The Hadoop Framework}

Hadoop is Apache's free and open-source implementation of the MapReduce framework. Apache Hadoop offers reliable, scalable, parallel and distributed computing scaling up from a single server to a network of multiple computers[4]. It was developed with the purpose of having a data store that allow organisations to leverage big data analytics with cost efficiency in mind.\\

%There are two main components of the Hadoop Framework: Storage and Processing

\section{Hadoop Architecture}

Hadoop follows a master/slave architecture design for data storage and distributed data processing using HDFS and MapReduce respectively. The master node for data storage is NameNode while the master node for parallel processing is the Job Tracker. The slave nodes are comprised of other machines in the Hadoop cluster which stores the data and performs the computations. Each slave node have a DataNode and a TaskTracker that synchronises the process respectively. \\

\begin{center}
 \includegraphics[width=10cm]{hadoop}\\
 \end{center}

The Hadoop system can be set up via cloud or locally. The cluster we will be running is a set up of 8 virtual machines.

\section{Storage - HDFS}

The storage component of the Hadoop architecture is known as the Hadoop Distributed File System (HDFS). The NameNode runs on the master node and manages metadata about the file system in a file named fsimage. This metadata is cached in main memory to provide faster access to the clients on read/write requests. The NameNode controls also manages the slaves by splitting files into chunks (default 64 megabytes) and distributing them across each DataNode in the cluster. The DataNodes are primary storage elements of HDFS where chunks of data are stored and replicated according to the instructions from the NameNode. Secondary NameNode is to periodically read the file system, log changes and applying them to the fsimage file. This will enable the NameNode to boot faster.\\

The main advantages of HDFS is data locality and fault tolerance. Data locality allow the nodes to manipulate the data they have access to which results in faster and more efficient processing while handling faults through the process of replicating files across each slave node[4]. \\

\includegraphics[width=8cm]{hdfs}

\section{Fault Tolerance in HDFS}

Fault tolerance refers to the ability for the system to handle unfavourable conditions, such as the failure of machine in the HDFS cluster. Fault is handled through the process of replica creation, which is to replicate the data on different machines in the HDFS cluster. So whenever a machine fail, the data can be accessed from other machines in which the same copies of data were created.

Suppose the user stores a file XYZ. HDFS will break this file into blocks, say A, B, and C. Let’s assume there are four DataNodes, say D1, D2, D3, and D4. HDFS creates replicas of each block and stores them on different nodes to achieve fault tolerance. For each original block, there will be two replicas stored on different nodes.

\includegraphics[width=8cm]{faulttol}

Let the block A be stored on DataNodes D1, D2, and D4, block B stored on DataNodes D2, D3, and D4, and block C stored on DataNodes D1, D2, and D3.

If DataNode D1 fails, the blocks A and C present in D1 are still available to the user from DataNodes (D2, D4 for A) and (D2, D3 for C).

Hence when one node breaks down, there won't be data lost. 

%insert diagram

\section{Processing - MapReduce}

In a MapReduce job, the input is broken down into multiple chunks which are processed by the map phase and then the output of the map phase is passed as input to the reduce phase. The input and output files are stored in the file system, while the output of the map phase (known as intermediate results) are only stored temporarily in the process. As the process runs in parallel, the reduce task on specific nodes may begin once its map tasks is complete rather than waiting on all map tasks to complete.\\

Similar to HDFS, the MapReduce process also utilises the master/slave architecture in which the JobTracker runs on the master node while the TaskTracker runs on each slave node. \\

\includegraphics[width=8cm]{jobtracker}

The JobTracker monitors the MapReduce tasks carried out by the TaskTracker running on the slave nodes. The user will only interact with the master node by submitting jobs to the JobTracker. The JobTracker then locates and submits the jobs to the TaskTracker. The slave nodes are monitored by the JobTracker through heartbeat signals to determine whether a node has failed. The JobTracker is a point of failure for the Hadoop MapReduce service and if it goes down, all jobs will be stopped. \\

The TaskTracker runs on the slave nodes in a cluster and receives jobs from the JobTracker to execute the MapReduce tasks. Each TaskTracker has an allocation of task slots which indicate the number of tasks it can accept. The JobTracker will delegate jobs to the TaskTracker based on the number of available/empty slots while the TaskTracker will periodically send heartbeat signals to inform the JobTracker of any issues. If one task dies either during the map or reduce phase the JobTracker is able to restart and repeat the task by assigning it to another node. Hence TaskTracker failure is not considered fatal as tasks can be reallocated when it becomes unresponsive.\\

In the case the JobTracker does not receive any heartbeat from a TaskTracker for a period of time (default is set to 10 minutes), the JobTracker will know that the worker has failed. The JobTracker will reschedule all pending tasks to another TaskTracker. All the completed map tasks need to be rescheduled if they belong to incomplete jobs because the intermediate results may not be accessible to the reduce task. 

\chapter{Streaming in Hadoop}

\section{What is Streaming}

\includegraphics[width=14cm]{streaming}\\

When we think of a stream we often picture water flowing through a fixed channel or pipe. Likewise in computer programming, it refers to the flow of data. A stream is basically a sequence of data bits made available over time from one node to another. From one node to the next, each channel involves depicted by the arrow is a pipe, which includes standard input and standard output. The program can get input from a source by reading in data as standard input. This input is the standard output of the source. Likewise, the standard output of the program is read into the destination as standard input. 

We refer to the complete process as a stream. In terms of Hadoop Streaming, the data is streamed from HDFS to another program and back to HDFS.

\subsubsection{Example}

We can look at a basic example to understand how streaming works. We have a comma-separated values (csv) file stored in HDFS that we will stream into R.


The first command creates a pipe with connects the csv file in HDFS with R. The file is read into the R using the read.table function and saved into an R data frame object called 'd'. Note that we specified that our file contained a header, and each field is separated by a comma and the number of rows we read in is 5. Lastly, we output the data using the print function. 

\section{Hadoop Streaming}

Hadoop streaming makes use of the streaming process which we discussed earlier. It is a utility that comes with the Hadoop distribution which allow users to create and run MapReduce jobs to be executed in the Hadoop cluster using other languages. By default the Hadoop MapReduce framework is written in Java but it utilizes Unix streams to pipe between Hadoop and our MapReduce program so we can use any language which can read standard input and write to standard output for our MapReduce program. MapReduce programs can be written in multiple languages such as R, Python, Perl, PHP, C++, etc. The utility is packed in a JAR file. 

Using the utility, we can create executable scripts to run MapReduce Jobs. Furthermore, we can create executable scripts to run the mapper and reducer functions which are passed to Hadoop streaming. The utility creates map and reduce jobs and submits them to the cluster which can then be monitored, thus enabling a person to write MapReduce job in the language of their choice without having any knowledge of Java.

\section{Syntax}

The syntax below can be used to run MapReduce jobs written in a different language to process data using the Hadoop MapReduce framework.\\

\begin{verbatim}
$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-streaming.jar 
    -input myInputDirs 
    -output myOutputDir 
    -mapper wc -l 
    -reducer awk '{n=n+$1}END{print n}' 
\end{verbatim}

The input command is used to provide the directory fo the input where the output command is used to provide the output directory. The mapper command is used to specify the executable mapper class while the reducer command is used to specify the executable reducer class. 

In the example above, myInputDirs is the input directory for the mapper and myOutputDir is the output directory for the reducer. The mapper executes the word count linux command on the input data which is fed to the reducer which aggregates result of the word count as a total and then returns the output to the output directory. 

\section{Implementation of Hadoop Streaming}

Let us now explore how Hadoop streaming works.\\

\includegraphics[width=14cm]{hadoopstreaming1}\\

In the above example image, we can see that the flow shown is a basic MapReduce job. Our main focus will be on the processes in the Mapper and Reducer stream.

\subsection{Input Format}

We have an Input Reader which defines the input specifications for the MapReduce job. it is responsible for reading the input data producing the structure for the Map process. We can read data from any source which may be structured or unstructured. This includes csv formats, delimiter formats, from a database table, images, audios etc. The only requirement to read all these types of data is that we have to create a particular input format for that data with these input readers. The input reader contains the complete logic about the data it is reading. We want to be able to structure the data in a way that the mapper can interpret and process. Hence we have to specify the logic which defines the structure of the input.

\subsection{Mapper \& Reducer}

Mappers and Reducers are the Hadoop servers that run the Map and Reduce tasks respectively. The tasks involve reading in the data, processing the data through user defined scripts, and outputting it. Through Hadoop Streaming, the data is read into a different program where it can be processed using different language, and output back into the default Hadoop server.

When the mapper is initiated, the input is broken down into multiple blocks known as input splits defined by the Input Format. Each mapper task associated with each block will convert its input into lines to the standard input (stdin) of the process. The Mapper essentially runs the formatter function on the input data from the standard input to convert the file into a format that can be executed by the program. The map program then executes the mapper function that performs the calculation/ reduce job. Once the script is executed by the map program, the mapper collects the line-oriented outputs from the standard output (stdout) of the process and converts each line into structured key/value pairs known as the intermediate values. The intermediate values are structured in a way where the prefix of a line up to the first tab character is the key value by default and the rest of the line will be the value associated with the key. If there is no tab character in the line, then the entire line is considered as the key with no value associated. After all the mappers tasks are complete, the framework shuffles and sorts the results. The output of the mapper is fed to the reducer task. 

The reducer runs the same process as the mapper where the intermediate values are now parsed into the reduce program. The reducer runs the formatter function on the input key/value pairs followed by the execution of the reducer function that performs the calculation/ reduce job. Once the job is executed by the reduce program, the reducer converts its input key/value pairs into lines and converts into tab separated key-value pairs. The output of the reducer is the final output of the MapReduce job. 

\subsection{Output Format}


%%%%write on output format


\section{Advantages}

Below are some of the advantages of using Hadoop Streaming.

\begin{enumerate}
\item Availability - the utility comes with the Hadoop distribution hence does not require further installation of softwares.
\item Learning - relatively easy to learn as it requires basic unix coding.
\item Reduce Development Time - it is much quicker to write mapper and reducer scripts/functions whereas using native Java MapReduce application is more complex as it requires the application to be complied, packaged, and exporting the JAR file.
\item Faster Conversion - it takes very little time to convert data from one format to another using Hadoop streaming especially when the input and output formats are specified.
\item Testing - input and output data can be tested quickly by using it with Unix or command line tools.
\end{enumerate}

\chapter{HMR Hadoop MapReduce Package for R}

We will be using R to stream and execute MapReduce jobs in the Hadoop cluster. \\

HMR is a package developed in R which acts as an interface which give users the ability to execute Hadoop MapReduce jobs directly through R. The MapReduce job can be completely submitted using R without the user having to access Hadoop or code in Java. The package enables Hadoop streaming as it allows the user to specify the Input Format, the Map and Reduce function, and the Output Format through R functions and R data structures such as matrices and data frames.

High efficiency is achieved through parsing data in binary and utilises chunk-wise processing as well as automated conversion to and from R objects. This enables a smooth and efficient transition between R and Hadoop. There are also other packages available in R which uses the traditional key-value operations such as the \pkg{rmr2} package.

\section{Examples}

Using the current package, we will perform some simple calculations using the taxi data set store in HDFS. We can access the files through Unix shell and simple Bash commands.

\begin{verbatim}
hadoop@hdp:~$ hadoop fs -ls taxi
\end{verbatim}

This data set contains 19 attributes ranging from vendor ID, pickup/drop off times, distance, location, payment, number of passengers, etc. \\

The following R script streams the MapReduce process through the HMR function in R. The script will calculates the number of lines/records for January 2015. with the map function being the shell command for counting the number of lines. The script uses the default formatter as no formatters have been specified.

\begin{verbatim}
hmr(hinput("taxi/2015/01"), map="wc -l", reducers=0, wait=FALSE)
\end{verbatim}

The following R script also performs the same line count calculation. However, the formatter has been specified which only include the first 5 fields of the data set. Instead of using shell command as the previous script, the mapper is using a R function "nrow" which also calculates the numbers of rows. The difference is that the number of reducers has been set to 1. This will ensure that the we only receive one output.
 
\begin{verbatim}
hmr(hinput("taxi/2015/01",
           formatter=function(o) dstrsplit(o, list(vendor=1, NA, NA, 
           	pass=1, dist=1), sep=",", strict=FALSE)),
    map=function(d) nrow(d), reducers=1, wait=FALSE)
\end{verbatim}

The output is saved to a directory in HDFS and can be access with the unix command hadoop fs -ls /folder name and printed using hadoop fs -cat /folder name. As we did not specify a reduce function, we return the output of the mapper which is the row count. Each block of data is assigned to a single mapper program and this case we have 15 mapper producing 15 blocks of output. \\ 

\begin{verbatim}
hadoop@hdp:~$ hadoop fs -cat /tmp/io-hmr-temp-30397-0831adbdb7fb1e3e/*
686375	
861469	
861493	
861503	
861529	
861574	
861610	
861626	
861649	
861652	
861678	
861681	
861691	
861715	
861742	
\end{verbatim}

\section{Concept of Formatting - Specifying Structure for Input Data}

When data is streamed from an external source into R, we need to define the structure of the data file for R to interpret. \\

Below is an example of parsing data from local disk to R. We use the R function read.table to illustrate how data is parsed. We added specifications on how the data is read into R. To ensure the data is read in correctly, we can specify the header, the separator symbol for each attribute, the data type for each attribute and the number of rows. 

\begin{verbatim}
> d <- read.table(file="data/taxi.csv", header=TRUE, sep=",", 
				colClasses=c("integer", "numeric", logical", rep("character", 6), nrows=5)
> d[,c(1,3,4,5)]
  VendorID tpep_dropoff_datetime passenger_count trip_distance
1        2   2015-01-15 19:23:42               1          1.59
2        1   2015-01-10 20:53:28               1          3.30
3        1   2015-01-10 20:43:41               1          1.80
4        1   2015-01-10 20:35:31               1           .50
5        1   2015-01-10 20:52:58               1          3.00
\end{verbatim}

The consequence of incorrectly parsing data into R will lead to issues when we try to compute on the data later on. 

\section{Formatter in HMR}

When we stream data from Hadoop, we are essentially creating a connection between HDFS and R. The above example works great when we can read in data from a structured file. What if our data file is not structured in a way we can easily read, or what if the file is too big for us to read efficiently. 

The solution is to write a R function that defines the structure of the input data we want to parse. We call this the Formatter. This is a lot more powerful than data input functions which requires the file to be structured. With the formatter function, we can parse in any type of data. In the HMR package, the formatter function, we provide the specifications, or in other words, the metadata which is the data that provides information about our data file. This includes information like the separator we use for each field, how many fields we have, the classes/ data types of our fields, etc.

\section{Hadoop Streaming in R}

We have discussed the idea of how Hadoop Streaming works, let us now see how we can stream the MapReduce process in R using the HMR package. The HMR package is essentially a Hadoop streaming API which runs in R. This allows the user to run MapReduce jobs and write formatter functions in R as opposed to writing scripts in the native language Java. \\

The first step is to specify the connection which is to define the HDFS path and input source. Once we have the connection established, the data can be parsed into R. The data is separated into blocks consisting of lines/rows referred to as chunks and the data is parsed as raw bytes which is much more efficient than parsing data as character values or any other data types. 

For R to process the MapReduce program, we will need to format each chunk of data to a structure that R can interpret. This is the next step to specify the formatter functions for the mapper and the reducer. In R, this involves writing function that will convert the raw bytes into a rectangular structure with rows of records and columns of attributes (fields). Within each column, we can also specify the class/data type associated with each field. The default classes in R include integer, numeric, logical, character, complex, POSIXct, etc. The structure of the data as a result of executing the formatter should return an R object which is typically in the form of a matrix or data frame.

The next step is to specify the mapper and reducer functions for the map and reduce process. The nice thing here is that our functions are no longer in Java, we can simply write these function using R code to process and compute on the R data object which we have just converted. Each chunk of data will be associated with each map process. The mapper takes in the R object consisting of values and executes the user defined mapper function. The mapper function identifies the keys from the values associated with it and generates the output R data frame with the key and value/s separated by a tab delimiter. The data frame generated by the mapper is converted back to raw bytes before it is parsed onto the reducer as input.

If the reduce function is not specified, the output will be parsed back to Hadoop HDFS. We will just receive the output of the mapper which is the shuffle and sorting of the key and its associated values without being reduced/aggregated. If we have specified the reduce function, the process follows the same process as the mapper. The reducer executes the formatter function and parse the data form the mapper output. Again, it is split into chunks, however this is dependent on how many reducers we have specified. The number of reducers will determine the number of chunks and the number of outputs? %check this%  
Each chunk of data will be associated with each reduce process. The reducer takes in the R object consisting of key-value pairs and executes the user defined reduce function. The objective is to take all the values that have the same key assigned to a single reducer which then aggregates/combines the values for that key. The output generated by the reducer is converted back to raw bytes before it is parsed back to Hadoop and stored in HDFS in a set directory. 

Mapreduce generates a function that is passed to Hadoop to execute?

\section{Leveraging\pkg{iotools} for Efficiency}

The HMR package is highly efficient as it leverages the \pkg{iotools} package in \R. It is a set of tools for quickly importing and processing datasets using any data source. The functions packaged in \pkg{iotools} are comparatively faster than the native functions in R as well as other packages such as bigmemory, readr, foreach, etc. 

The two functions chunk.reader and read.chunk are functions used in the HMR package to read data as multiple lines as opposed to reading line by line sequentially. Chunk reader reads by default 32Mb (as default) of lines from a binary connection and stores it as an R object while preserving the integrity of the lines. Chunk.read then converts this object into a raw vector for each subsequent block of 32Mb. By parsing blocks of lines rather than single lines, we are able to achieve higher efficiency for data streaming.

Other core functions in the \pkg{iotools} package used in the HMR package are \code{mstrsplit} and \code{dstrsplit}. \code{mstrsplit} takes a raw or character vector and converts it into a character matrix according to the separators, while \code{dstrsplit} takes the vector and convert it into a dataframe instead. These two functions are primarily used to convert the raw output from the chunk functions into a R object which we will feed into our MapReduce process in R. These functions are able to minimize the copying of data and avoid the use of intermediate strings to improve performance.

The other function we will be leveraging is \code{as.output} which is the reverse of \code{mstrsplit} and \code{dstrsplit}. \code{as.output} essentially converts our R object back into the raw bytes while preserving the metadata/structure of our data. This function is used to output our data after we have executed our map and reduce programs. 

\section{Current Approach - Formatting Logic}

In order for us to execute MapReduce jobs, we will require a server/cluster with Hadoop set up. The file path/connection defines the HDFS file path and input source. HMR can take two types of input objects: \code{hpath} and \code{hinput}. Hpath only contains the path of the input whereas hinput has an additional formatter argument which allows the user to specify the class of the input variables as a vector or a list. This will require the user to have some familiarity or knowledge of structure of the data. Once the connection is specified, input is read into R using chunk.reader and chunk.read from \pkg{iotools}. 

The current logic allows the user to specify the formatter for both the mapper and reducer as a list or the map or reduce formatter as a vector. If a single formatter function is specified, the same formatter will be used for both the map and reduce process. If no list or vector is specified for the formatter, then HMR defaults to the default formatter. The default formatter leverages the \code{mstrsplit} function that takes a raw vector which in our case is our data from HDFS and convert it into a matrix with a key and value structure. The key is the first column of the matrix separated from the values by a tab characters, however the values are returned as character types which is undesirable. The downside of this is that "character" data type requires more storage compared to other data types such as logical and integers. Having the incorrect data type also means we will need to convert our values to the correct type before we can compute on it. This can be resolved by converting the data type in our map and reduce functions.

\section{Example of Formatter Function}

\begin{verbatim}
hinput("taxi/2015/01",
           formatter=function(o) dstrsplit(o, list(vendor=1,NA,NA,pass=1,dist=1), 
           sep=",", 
           strict=FALSE)
\end{verbatim}

\section{Motivation for Enhancements}

So far we have looked at running the MapReduce program with only the Mapper. To format the input data for the mapper is relatively simple as we have visibility of the data which we can easily infer the class/data type. However, this is not as simple for the reducer as we have no visibility of the mapper output unless we run multiple processes, i.e. run the whole MapReduce process with the mapper only then use the output to format our input data to the reducer. We want to be able to run MapReduce as one process as opposed to having to resort to trial and error. 

Hence our motivation is to automatically infer the class/data types for a structured data set.

\chapter{Package Enhancement - Automation of Formatters}

% talk about motivation 

As we have stated above, the goal of our project is to infer the input formatters for the mappers and the reducer by automatically detecting the class of the attributes from the input. 

We know the importance of formatters with regards to writing the map and the reduce functions to execute the MapReduce job. Currently there is no way to infer the formatter for the reducer, hence it is particularly difficult to write the reduce function. Writing the formatter for the mapper is relatively easy as we can access the input data with ease, but it is not the same case for the reducer unless we know the output of the mapper.

The current approach allows the user to specify the formatter for the mapper and the reducer or the package defaults to the default formatter which is highly inefficient as it reads the variables as strings/characters. This may lead to running additional MapReduce jobs in order to reach the final desired output. It also adds more coding to the reducer function in the case where the data type may needs to be converted appropriately for the reducer to run successfully.

%%%The issue with the default formatter is that it is highly inefficient to read in data as string/characters. It will also require additional coding in the map and reduce functions to convert the data types in order to run the job.

%gives user customisation

\section{Concept of Auto Detection}

We explored how we can write function to infer the data types for our input attributes and now we want to automate this as a feature for HMR.

The idea is that we take a sample of the input data and use that infer the data type for the attributes and return the formatter function to the formatter in HMR. There are two approaches that we can take as we can create the formatters before executing our MapReduce job or we can create the formatters in the Map and Reduce tasks. We refer to the ahead of time as the "Static" approach and the in between approach as the "Dynamic" approach. The two approaches will have their advantages and drawbacks dependent on type of data that we are processing. 

\subsection{Auto Formatting - Mapper}

As we mentioned in the chapter above, to infer the data type for the mapper is relatively easy as we have access to the input data from HDFS. The goal is to automatically generate the formatter for the Map process. We will explore idea behind the two formatting approaches for the mapper.

\subsubsection{Dynamic Approach}

The dynamic approach formats the input data during the Mapper stage. The input data for the Mapper is structured in a way to only contain values. The dynamic approach reads a sample of the raw input data and returns a vector containing the column types for each value/attribute. This vector containing the column types is passed to the formatter function which is assigned to the map formatter allocated for each chunk. In our MapReduce job we will have a vector of column types for each Mapper which generates a separate formatter function containing the metadata of our input data for each data chunk.

\subsubsection{Static Approach}

The static approach formats the input data ahead of time, in other words, before the MapReduce job begins to execute. The static approach first creates a connection to the HDFS directory and reads a sample from the raw input data to also generate a vector containing the column types for each value/attribute. The difference is that we only execute and obtain one vector of column types. This approach generates a single formatter function which contains the metadata of our input data to be executed across all the mappers in the job.

\subsection{Auto Formatting - Reducer}

To infer the data type for the Reducer is quite difficult as we do not have immediate access to our input data which is also the output of the Mapper. Unlike the input data for the mapper, it is not stored on disk. The goal is to automatically generate the formatter for the Reduce process. We will explore idea behind the two formatting approaches for the reducer.

\subsubsection{Dynamic Approach}

The dynamic approach formats the input data during the Reduce stage. The dynamic approach for the Reducer is similar to the mapper as it reads the data during the executing of the Reduce phase. The difference for the Reducer is that the input data now contains a key-value structure which the formatter function will need to define. The approach reads a sample of the raw input data and returns a vector containing the column types for each value/attribute. This vector containing the column types is passed to the formatter function which is assigned to the reduce formatter allocated for each Reducer. In our MapReduce job we will have a vector of column types for each Reducer which generates a separate formatter function containing the metadata of our input data depending on the number of Reducers specified. 

\subsubsection{Static Approach}

The static approach for the Reducer formats both the input data of the Map phase and the Reduce phase prior to executing the MapReduce job. What we are essentially doing is running a MapReduce job on a sample of the complete data to obtain the formatter functions that we can then apply to the job. In this approach, we again create a connection to the HDFS directory to parse a sample of the raw input data with the column types for each value/attribute. At this point the vector of column types are only associated with the input for the Mapper, hence we run the Mapper process on the subset we obtained. From the output of Mapper, we can obtain the vector containing the datatypes for the values/attributes which we assign to the formatter function for the Reducer. This way the static approach creates a single formatter function for both the Map phase and the Reduce phase.



 that can turn input data stored in raw bytes to an R object that can be read and processed by our Mapper program. 

The idea is to read a reasonable size of data from HDFS and convert it into a R object usually in the form of a matrix with lines as the rows and attributes as the columns. From this matrix, we can identify the data type associated with each column. 



to infer the data type for each column. The first solution is to detect the 

For the mapper process, we take the input data from Hadoop 

For the reducer process, we take the input data from the output of the mapper.

The easiest way is to take the first megabyte or specified number of lines to perform the detection. The drawback with this approach is that we have to assume the data types will be consistent across the complete data set. 

After we have the subset in the form of raw bytes, we convert it into a matrix separated by each variable of the data set. In the matrix form, we can detect the data type in each column as logical, integer, numeric, complex, character or factor. It should return a vector of the data type for each variable based on each column of the matrix. 



auto detection - infer metadata from data file 
what are different places we want to Use this
input to mapper and input to reducer (reducer also need to take care of the key)
reducer - recognised the key and values. 
what am i trying to solve(
  mapper need formatter
  reducer I need the output of mapper will be to format the input data
  challenges: doesnt exist on disk
what and how for mapper then what and how for reducer

Before we can infer the data type, we need to know the structure of our input data, how each field is separated. Ideally we want to work with data that does not have a header or remove it. 

\section{Implementation in HMR}

\section{New Approach - Two Approaches}

%talk about the idea of running in dynamic and in advance

The enhancement provides automatic detection of the formatter for the mapper and the reducer in the absence of user specified formatters. The additional argument included in the HMR function is "autoformatter" which is set to TRUE for dynamic detection, FALSE for static approach and defaults to NULL if the user wish to specify their own formatters.

There are two approaches for detecting the formatter. The original approach is to detect the formatter in advance which we refer to as the "static" approach. The alternative approach is to run the on each chunk during the MapReduce process. This is referred to the "dynamic" approach.

\subsection{Dynamic Approach}

With the dynamic approach, it is a lot simpler to detect as we already have the data available in raw bytes so all we need to do is to find the data types for the formatter. We can simply detect both the formatter for the mapper and the reducer using the same function as it runs within the process. 

The function named "coltypes" which takes the data in raw bytes, the specified column separator, the row name separator, specified number of lines, and the header. Using the mstrsplit function from\pkg{iotools} and the specified arguments, we can convert the raw data into matrix object in R. With each column of the matrix, we can apply the functions type.convert to create the appropriate data object across the rows and call the class function to determine the object class of each column/attribute. Using matrix operation functions we can operate on each of the columns and returns a vector containing the data type for each variable in the data set.

advantages: each chunk may vary, 
disadvantage: no header

\begin{verbatim}
  ## dynamic approach
  coltypes <- function(r, sep=formsep, nsep='\t', 
                       nrowsClasses=25L, chunksize=size, header=TRUE) {
    if (sum(r==10) < nrowsClasses) {
      nrowsClasses = sum(r==10)
      r <- r[1:chunksize]
      r <- r[1:tail(which(r==10),1)]
    }
    subset = mstrsplit(r, sep=sep, nsep=nsep, 
    			nrows=nrowsClasses, skip=header)
    colClasses = apply(subset, 2, function(x) 
    				class(type.convert(x, as.is=TRUE)))
    if (header) {
      col_names = mstrsplit(r, sep=sep, nsep=nsep, nrows=1)
      if ((length(col_names) - 1 == length(colClasses)) && !is.na(nsep))
        col_names = col_names[-1]
      names(colClasses) = col_names
    }
    colClasses
  }
\end{verbatim}

<<echo=FALSE, include=FALSE>>=
    library(iotools)
library(knitr)
    options(prompt = "> ", continue="  ", useFancyQuotes = FALSE)
opts_chunk$set(comment = NA,
               prompt = TRUE,
               fig.align = 'center',
               fig.show = 'hold',
               tidy = FALSE,
               size = 'footnotesize',
               cache = FALSE,
               cache.path = 'MyKnitrFigs/mycache/',
               background = rgb(0.9803922, 0.9803922, 0.8235294), # lightgoldenrodyellow
               highlight = TRUE,
               continue = "  ")  # Usually a "+"

r <- charToRaw("VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,pickup_longitude,pickup_latitude,RateCodeID,store_and_fwd_flag,dropoff_longitude,dropoff_latitude,payment_type,fare_amount,extra,mta_tax,tip_amount,tolls_amount,improvement_surcharge,total_amount
2,2015-01-15 19:05:39,2015-01-15 19:23:42,1,1.59,-73.993896484375,40.750110626220703,1,N,-73.974784851074219,40.750617980957031,1,12,1,0.5,3.25,0,0.3,17.05
1,2015-01-10 20:33:38,2015-01-10 20:53:28,1,3.30,-74.00164794921875,40.7242431640625,1,N,-73.994415283203125,40.759109497070313,1,14.5,0.5,0.5,2,0,0.3,17.8
1,2015-01-10 20:33:38,2015-01-10 20:43:41,1,1.80,-73.963340759277344,40.802787780761719,1,N,-73.951820373535156,40.824413299560547,2,9.5,0.5,0.5,0,0,0.3,10.8
1,2015-01-10 20:33:39,2015-01-10 20:35:31,1,.50,-74.009086608886719,40.713817596435547,1,N,-74.004325866699219,40.719985961914063,2,3.5,0.5,0.5,0,0,0.3,4.8
1,2015-01-10 20:33:39,2015-01-10 20:52:58,1,3.00,-73.971176147460938,40.762428283691406,1,N,-74.004180908203125,40.742652893066406,2,15,0.5,0.5,0,0,0.3,16.3
1,2015-01-10 20:33:39,2015-01-10 20:53:52,1,9.00,-73.874374389648438,40.7740478515625,1,N,-73.986976623535156,40.758193969726563,1,27,0.5,0.5,6.7,5.33,0.3,40.33
1,2015-01-10 20:33:39,2015-01-10 20:58:31,1,2.20,-73.9832763671875,40.726009368896484,1,N,-73.992469787597656,40.7496337890625,2,14,0.5,0.5,0,0,0.3,15.3
1,2015-01-10 20:33:39,2015-01-10 20:42:20,3,.80,-74.002662658691406,40.734142303466797,1,N,-73.995010375976563,40.726325988769531,1,7,0.5,0.5,1.66,0,0.3,9.96
1,2015-01-10 20:33:39,2015-01-10 21:11:35,3,18.20,-73.783042907714844,40.644355773925781,2,N,-73.987594604492187,40.759357452392578,2,52,0,0.5,0,5.33,0.3,58.13
1,2015-01-10 20:33:40,2015-01-10 20:40:44,2,.90,-73.985588073730469,40.767948150634766,1,N,-73.985916137695313,40.759365081787109,1,6.5,0.5,0.5,1.55,0,0.3,9.35
1,2015-01-10 20:33:40,2015-01-10 20:41:39,1,.90,-73.988616943359375,40.723102569580078,1,N,-74.00439453125,40.728584289550781,1,7,0.5,0.5,1.66,0,0.3,9.96
1,2015-01-10 20:33:41,2015-01-10 20:43:26,1,1.10,-73.993782043457031,40.751419067382812,1,N,-73.9674072265625,40.757217407226563,1,7.5,0.5,0.5,1,0,0.3,9.8
1,2015-01-10 20:33:41,2015-01-10 20:35:23,1,.30,-74.00836181640625,40.704376220703125,1,N,-74.009773254394531,40.707725524902344,2,3,0.5,0.5,0,0,0.3,4.3
1,2015-01-10 20:33:41,2015-01-10 21:03:04,1,3.10,-73.973945617675781,40.760448455810547,1,N,-73.997344970703125,40.735210418701172,1,19,0.5,0.5,3,0,0.3,23.3
1,2015-01-10 20:33:41,2015-01-10 20:39:23,1,1.10,-74.006721496582031,40.731777191162109,1,N,-73.995216369628906,40.739894866943359,2,6,0.5,0.5,0,0,0.3,7.3
2,2015-01-15 19:05:39,2015-01-15 19:32:00,1,2.38,-73.976425170898437,40.739810943603516,1,N,-73.983978271484375,40.757888793945313,1,16.5,1,0.5,4.38,0,0.3,22.68
2,2015-01-15 19:05:40,2015-01-15 19:21:00,5,2.83,-73.968704223632812,40.754245758056641,1,N,-73.955123901367188,40.786857604980469,2,12.5,1,0.5,0,0,0.3,14.3
2,2015-01-15 19:05:40,2015-01-15 19:28:18,5,8.33,-73.863059997558594,40.769580841064453,1,N,-73.952713012695312,40.785781860351563,1,26,1,0.5,8.08,5.33,0.3,41.21
2,2015-01-15 19:05:41,2015-01-15 19:20:36,1,2.37,-73.945541381835938,40.779422760009766,1,N,-73.980850219726563,40.786083221435547,1,11.5,1,0.5,0,0,0.3,13.3
2,2015-01-15 19:05:41,2015-01-15 19:20:22,2,7.13,-73.874458312988281,40.774009704589844,1,N,-73.952377319335938,40.718589782714844,1,21.5,1,0.5,4.5,0,0.3,27.8
2,2015-01-15 19:05:41,2015-01-15 19:31:00,1,3.60,-73.976600646972656,40.751895904541016,1,N,-73.998924255371094,40.714595794677734,2,17.5,1,0.5,0,0,0.3,19.3
2,2015-01-15 19:05:41,2015-01-15 19:10:22,1,.89,-73.994956970214844,40.745079040527344,1,N,-73.99993896484375,40.734649658203125,1,5.5,1,0.5,1.62,0,0.3,8.92
2,2015-01-15 19:05:41,2015-01-15 19:10:55,1,.96,-74.000938415527344,40.747062683105469,1,N,-74.003562927246094,40.735511779785156,1,5.5,1,0.5,1.3,0,0.3,8.6
2,2015-01-15 19:05:41,2015-01-15 19:12:36,2,1.25,-74.002777099609375,40.717891693115234,1,N,-74.007919311523438,40.704219818115234,1,6.5,1,0.5,1.5,0,0.3,9.8")


<<>>=
cat(rawToChar(r))
@ 

<<>>=
sep=","; nsep='\t'; nrowsClasses=5L; header=TRUE
(subset <- mstrsplit(r, sep=sep, nsep=nsep,
 nrows=nrowsClasses, skip=header))
@

<<>>=
apply(subset, 2, function(x) class(type.convert(x, as.is=TRUE)))
@ 
\subsection{Static Approach}

The static approach detects the formatter ahead of the time prior to executing the MapReduce job. As we are using a subset of the complete data set in advance, we will need to create a pipe to the HDFS location and read in the specified size of the sample data to perform the detection of the formatters. Using chunk.reader and chunk.read from \pkg{iotools} we can read in the connection and convert to a raw vector as input for the mapper. With the raw vector, we can leverage the coltypes function from the dynamic approach to detect the formatter for the mapper.

The challenge for this approach is that we have to obtain the intermediate key-value pairs in order to automatically detect the formatter for the reducer. Hence there is an extra step which is to execute the mapper first using the subset to generate the mapper output that we will feed into the reducer as input. As the output of the mapper is an R object, we will need to convert it back to a raw vector using as.output from the \pkg{iotools} package. From here we can use the same coltypes function to detect the formatter for the reducer. The output from the detection will be a vector containing the data type for each variable in the data set.


The risk with this approach is data may be incomplete, i.e. may be logical due to missing values. More suitable for known structure.

\begin{verbatim}
  ## new static approach
  guess <- function(path, chunksize=size, header=TRUE, map) {
    f <- pipe(paste("hadoop fs -cat", shQuote(path)), "rb")
    cr <- chunk.reader(f)
    r <- read.chunk(cr, chunksize)
    colClasses = coltypes(r)
    close(f)
    if (!missing(map)) {
      m = map(dstrsplit(r, colClasses, sep=formsep, skip=header))
      c = coltypes(as.output(m), header=FALSE)
      if (length(c) == length(names(m)))
        names(c) = names(m)
      rm(list=c("cr", "r", "f"))
      list(map=function(x) dstrsplit(x, colClasses, 
      	sep=formsep, skip=header), 
           reduce=function(x) dstrsplit(x, c, sep="|", 
           	nsep="\t", skip=FALSE))
    }
    else function(x) dstrsplit(x, colClasses, sep=formsep, skip=header)
  }
  \end{verbatim}

\section{Formatter Logic}

\begin{verbatim}
  ## formatter logic
  map.formatter <- NULL
  red.formatter <- NULL
  if (!missing(formatter)) {
    if (is.list(formatter)) {
      map.formatter <- formatter$map
      red.formatter <- formatter$reduce
    } 
    else map.formatter <- red.formatter <- formatter
  }
  else {
    if (missing(autoformatter)) {
      if (inherits(input, "hinput"))
        map.formatter <- attr(input, "formatter")
      if (is.null(map.formatter)) map.formatter <- .default.formatter
      if (is.null(red.formatter)) red.formatter <- .default.formatter
    }
    else {
      if (isTRUE(autoformatter)) {
        map.formatter <- function(x) dstrsplit(x, coltypes(x), 
        	sep=formsep, skip=TRUE)
        if (!missing(reduce)) 
          red.formatter <- function(x) dstrsplit(x, 
          	coltypes(x, header=FALSE), nsep='\t')
      }
      else if (autoformatter==FALSE) {
        if (!missing(reduce)) {
          formatter <- guess(paste0(input,"/*"), map=map)
          map.formatter <- formatter$map
          red.formatter <- formatter$reduce
        }
        else map.formatter <- guess(paste0(input,"/*"))
      }
    }
  }
  \end{verbatim}



\section{Examples}

\begin{verbatim}
## shell command - line count with default map formatter
hmr(hinput("taxi/2015/01"), map="wc -l", reducers=0, wait=FALSE)
\end{verbatim}

\begin{verbatim}
hmr(hinput("taxi/2015/01"), #autoformatter=TRUE,
    map=function(d) c(A=nrow(d)),
    reduce=function(o) sum(as.numeric(o)),
    reducers=1, wait=FALSE)
\end{verbatim}

\begin{verbatim}
## new case
# what if I want the total passenger for each vendor
hmr(hpath("taxi/2015/01"), autoformatter=FALSE,
    map=function(d) t(simplify2array(tapply(o[[4]], o[[1]], function(x)  c(sum(x), length(x))))),
    reduce=function(o) sapply(split(d, d$rowindex, function(o) sum(o[,2]) / sum(o[,3]))),
    reducers=1, wait=FALSE)
\end{verbatim}


\begin{verbatim}
hmr(hpath("taxi/2015/01"), autoformatter=FALSE,
    map=function(d) c(d[[1]], d[[4]]),
    #reduce=function(o) tapply(o[[2]], o[[1]], function(x) round(mean(x), 2),
                              reducers=1, wait=FALSE)
\end{verbatim}


\chapter{References}
1. BIG DATA: Challenges and opportunities, Infosys Lab Briefings,
Vol 11 No 1, 2013. \\

2. Big Data And Hadoop: A Review Paper, Rahul Beakta, 2015 \\

 3. IBM Big Data analytics HUB,
www.ibmbigdatahub.com/infographic/four-vs-big-data

4. Hadoop, MapReduce and HDFS: A Developers Perspective, 
Mohd Rehan Ghazi, Durgaprasad Gangodkar, 2015

5. MapReduce: Simplified Data Processing on Large Clusters, 
Jeffrey Dean and Sanjay Ghemawat

6. iotools: High-Performance I/O Tools for R, 
Taylor Arnold, Michael J. Kane, and Simon Urbanek

%https://www.researchgate.net/publication/261450947_Converting_unstructured_and_semi-structured_data_into_knowledge

\end{document}
