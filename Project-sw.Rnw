% !TEX TS-program = make

% --------------------------------------------------------------------------------------
%                   LATEX TEMPLATE FOR DISSERTATION (HONS)
% --------------------------------------------------------------------------------------
\documentclass[11pt]{book}

\usepackage{amsfonts, amsmath, amssymb}  
\usepackage{times}
\usepackage[utf8]{inputenc}

%\usepackage[backref=page,pagebackref=true,linkcolor = blue,citecolor = red]{hyperref}
%\usepackage[backref=page]{backref}

\usepackage{graphicx}
\usepackage{url}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\setlength{\oddsidemargin}{1.5cm}
\setlength{\evensidemargin}{0cm}
\setlength{\topmargin}{1mm}
\setlength{\headheight}{1.36cm}
\setlength{\headsep}{1.00cm}
%\setlength{\textheight}{20.84cm}
\setlength{\textheight}{19cm}
\setlength{\textwidth}{14.5cm}
\setlength{\marginparsep}{1mm}
\setlength{\marginparwidth}{3cm}
\setlength{\footskip}{2.36cm}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textsf{ \bf #1}}
\newcommand{\R}{\textsf{R}}



\begin{document}
\pagestyle{empty}

%: ----------------------------------------------------------------------
%:                  TITLE PAGE: name, degree,..
% ----------------------------------------------------------------------

\begin{center}

\vspace{1cm}

%%% Type the thesis title below%%%%%%%%%%%%%%%%
{\Huge         Enhanced Scalable Distributed Computing - Hadoop MapReduce and R}

\vspace{25mm} 

\includegraphics[width=3.5cm]{logo}

 \vspace{35mm}

%%%%%Type Your Name Below%%%%%%%%%%%%
{\Large       Noah Zhang}

	\vspace{1ex}

Department of Statistics

The University of Auckland

	\vspace{5ex}

 %%%%%Typing Your Supervisors Name Below%%%%%%%%%%%%
Supervisor:             Simon Urbanek

	\vspace{30mm}

A dissertation submitted in partial fulfillment of the requirements for the degree of BSc(Hons) in Statistics, The University of Auckland, 2021.

\end{center}

\newpage


%: --------------------------------------------------------------
%:                  FRONT MATTER:  abstract,..
% --------------------------------------------------------------
\chapter*{Abstract}       
\setcounter{page}{1}
\pagestyle{headings}
% \pagenumbering{roman}

\addcontentsline{toc}{chapter}{Abstract}

MapReduce is a popular programming model for processing and analysing large data sets. The programs of MapReduce are parallel in nature, which can be implemented on large cluster of commodity machines. Thus it is very powerful for performing large-scale data analysis. \\

MapReduce allows the user to specify a map function that process values to generate a set of intermediate key-value pairs and a reduce function which merges all the intermediate key-value pairs with the same key. This process is enable in R through Hadoop Streaming. This is implemented using the\pkg{HMR} package which is a high performance MapReduce interface based on\pkg{iotools}. The performance comes from the ability to parse data in chunkwise operations as opposed to the classic row by row operations in the the\pkg{rmr} package.\\

In order to execute the Map/Reduce tasks in R, input data is required to be parsed as R objects. In other words, a formatter is required for both the Map and the Reduce program. The formatter is a R function which defines the structure of the data by taking the raw input and converting it into a structured key-value object. The formatter function can easily be written for the Mapper this can be inferred from the input data in HDFS. However, the formatter function for the Reducer is difficult to write as there is no immediate access to the Mapper output in the MapReduce program. The motivation behind the project is to enhance the features of the\pkg{HMR} package through automating the detection of the formatter function by inferring the input data of the Map and Reduce tasks.\\

The package, source code and documentation can be acessed through the following url:\\

\url{https://github.com/nzha371/hmrtest}

%: --------------------------------------------------------------
%:                  END:  abstract
% --------------------------------------------------------------


%: ----------------------- contents ------------------------
\setcounter{secnumdepth}{3} % organisational level that receives a numbers
\setcounter{tocdepth}{3}    % print table of contents for level 3
\tableofcontents            % print the table of contents
% levels are: 0 - chapter, 1 - section, 2 - subsection, 3 - subsection

%: --------------------------------------------------------------
%:                  MAIN DOCUMENT SECTION
% --------------------------------------------------------------
	
\chapter{Introduction}%    \chapter{}  = level 1, top level

In this day and age, we have the ability to collect and store large volumes of data, however, the question is whether we have the same ability to effectively parse and process data sets that are too big to contain on individual computers. Storage and processing power may not be an issue with the development in technology, but this method is not cost effective for most users and companies that wish to analyse their data. The alternative is divide the data and process them across multiple machines. Even though the processes are spread across multiple machines, they are run as one system. This is referred to the distributed computing model.\\

The article looks into computation of large data sets using Hadoop MapReduce. MapReduce is a processing technique and a program model for distributed computing based on java. The algorithm contains two important steps, Map and Reduce. The Map process (referred to as the Mapper) deals with the splitting and mapping of data followed by the Reduce process (referred to as the Reducer) which shuffle and aggregates the data that come out of the Mapper. The MapReduce model can be executed through Hadoop which is an open source software framework for storing data and running processes like MapReduce. The data is stored in the Hadoop Distributed File System (HDFS) which is a distributed file system which is used to scale a single Hadoop cluster to hundreds to even throughs of nodes (machines). Hadoop is also capable of running MapReduce programs written in various different languages such as R, Ruby, Python, C++. This is referred to as Hadoop Streaming.\\

The main focus of the article is related to Hadoop Streaming specifically through R. The HMR package is used to execute Hadoop MapReduce jobs in R. It is available on Github under \url{https://github.com/s-u/hmapred}. The purpose of the project is to explore methods to enhance the capabilities of HMR for executing MapReduce programs in R. The article explain concepts and idea of how streaming works and the current implementations using HMR. Then we shift our focus to the idea of formatters. Formatters define the structure of the input data to both the Map and the Reduce process and is crucial for the streaming and execution of the MapReduce job. We can easily infer the structure and data type of the values for the Mapper from the input in HDFS, however we have no option to do so for the Reducer input. This is due to system limitation where the Mapper output is not directly accessible in the process.\\

The next part of the articles focus on the concept and implementations of auto detecting the formatter for the Mapper and the Reducer. By doing so, we ensure that the job runs successfully without losing out too much on the efficiency of the process. We implement two approaches that can automatically detect the formatter functions for both the Mapper and the Reducer. They are referred to as the Dynamic approach which detects the formatters during the MapReduce job, while the Static approach detects the formatters ahead of time, which is before the execution of the MapReduce job. Examples and results are provided on various MapReduce algorithms using the two approaches.\\

Lastly we conclude on the results and effectiveness of the enhancement in HMR as well as providing future directions for further development.



\chapter{Concept of Big Data}

Big data concerns large amounts, complex and diverse sets of information that is growing at ever-increasing rates. The size of the data suggest that we will need to deviate from the traditional processing methodologies and to adapt an inexpensive and efficient way through distributed/parallel computing.\\

Today, we can find upwards of 800 million webpages providing documentation on big data. Enthusiasts believe that Big Data is the next big thing after Cloud\cite{Infosys}.

\section{Characteristics of Big Data}

The concept of big data is generally vague without a formal definition, however the general consensus is that there are specific attributes that define big data The four characteristics of big data are Volume, Velocity, Variety and Veracity\cite{Beakta15}. 

\begin{center}
\includegraphics[width=10cm]{bigdata}\\
\end{center}

The main characteristic that makes data “big” is its sheer volume. According to estimates by IBM, we can expect at least 40 Zettabytes (43 Trillion Gigabytes) of data to be created in 2020 [?????]. The volume of data sets being processed and analysed has reached sizes larger than terabytes and even petabytes. This suggests that data sets these days are becoming too large to process within a single desktop machine/processor. \\

Velocity is the speed at which data is generated. High velocity data is generated with such a pace that it may require certain distributed processing techniques. Good examples of high velocity data includes social media posts.\\

Variety is the source of the data which can be found in different forms such as text, numerical, images, audio and video records. The variety in the data will require distinct processing capabilities or algorithms to handle different formats. \\

Veracity is the quality of the data. Information may be volatile or incomplete seen in low veracity data sets containing a high percentage of meaningless data referred to as noise. On the other hand, high veracity data hold records that are valuable to analyse and contribute in a meaningful way to the overall results.

\section{Structure of Big Data} 

Big data can be categorised as unstructured or structured.\\
 
Unstructured data has its internal structure but it is not structured through pre-defined data models or schema. As it may comes in many different formats, it cannot be stored in relational databases which also becomes a real challenge for systems to process and analyse. The unstructured data may be stored within non-relational databases like NoSQL.\\

Structured data, in contrast, is usually stored and managed in relational databases with predefined data models. Examples of relational database applications with structured data include customer information, sales transactions, airline reservations systems, and billing systems. This type of structured data within relational databases can be accessed using Structured Query language (SQL).\\
 
 \section{Challenges around Big Data}
 
 \subsection{Converting Unstructured Data}
 
The challenge today is that bulk of the data we have and store do not come in a nice structure that we can easily access and perform analysis on. In managing unstructured data, many organisation simply collect and store significant volumes of unstructured information without having the ability to process and interpret it. This is due to the difficulty and cost of exploring it. However, it is not to say there is no way of processing unstructured data. There is simply not a standard, or one conventional method of processing such data. There are ways that structured data can be created from unstructured data through various processing methods. By converting to structured data, the data can be universally understood by the ordinary user, easily digestible by data programs as well as having the ability to transfer to other data tools. Hence there is the motivation to turn this unstructured data into structured data so we can be more efficient in terms of processing and analysis. \\

When converting from unstructured data to structured data, the questions we often ask is what data do we wish to extract, is it a word count index or is it a particular feature, topic or sentiment? Then we want to explore what the structure look like. Is it key value pairs, JSON, XML, or Tabular? In the processing, we analyse the data and retrieve a structured metadata that augments the original data source. The alternative is to extract and clean the data if we already know what features we wish to extract. 

\subsubsection{Example}

%% give example converting unstructured to structured

\subsection{Other Challenges}

Apart from the challenge to create structure from unstructured data, we are also concerned with the ability to process large concentration of data. Of course you may say that technology is improving at the same time, however, to have the biggest and strongest machine is not always ideal and cost-effective. Below are some alternative approaches that we can take to tackle the challenge around processing big data.

\subsubsection{Serial vs Parallel Computing}

In the early days of computing, programs were serial, which means a program consisted of a sequence of instructions where each instruction is executed one after the other. It ran from start to finish on a single machine.

Since then, parallel computing is developed as a means of improving performance and efficiency. In a parallel program, the processing is broken into parts where each part can be executed concurrently. The instructions from each part runs simultaneously on different CPUs. These CPUs can exist on a single machine or they can be distributed on a cluster of machines connected to a network. 

\subsubsection{Scaled Distributed Computing}

Distributed computing involves multiple computers, physical servers, virtual machines, containers or any other node that communicate and coordinate actions in order to appear as a single coherent system. In its most simplest form, it is a group of computers working together as a single computer to the end user. These machines have a shared state, operates in parallel and can fail independently without affecting the whole system's uptime. \\

The diagram below shows a cluster with 6 machines.

\includegraphics[width=10cm]{distributed} \\

For a single machine to handle more traffic, we can only scale vertically by upgrading the hardware of the machine. What a distributed system enables us to do is scale horizontally. To scale horizontally simply means to add more computers rather than upgrading the hardware of a single machine. This is significantly cheaper than vertical scaling. We will discuss further advantages of a distributed system in the chapters below.

\chapter{MapReduce Framework}

At present, with data being generated at an exponential rate, there is the need to deploy data intensive application and storage clusters in order to keep up with the amount of data. To handle such problem, Google developed the MapReduce programming model for distributed computing based on the Java language\cite{Dean04}. The programs of MapReduce are parallel in nature, thus are very useful for performing large scale data analysing using a distributed system with multiple machines in the cluster. 

\section{Concept of MapReduce}

There are multiple approaches for processing relatively smaller datasets, however larger datasets require a different approach specifically for data that is too big to fit in memory. The traditional approach to process data on a single machine is to break data into individual chunks which is then loaded and processed sequentially on the machine. 

MapReduce is a processing technique/programming model for distributed computing. The MapReduce program works in two phases, namely Map and Reduce. The Map tasks deal with the splitting and mapping of data while the Reduce tasks shuffle and aggregate the data.

The algorithm operates on key-value pairs, which means that it takes a set of input key-value pairs and produces a set of output key-value pairs. The input into the Mapper contains the key-value structure with the key as the row number. Realistically, the input to the mapper only consist of values in a key-value structure. In addition, the submits two algorithms for the Mapper and Reducer to compute on. The Mapper takes the input, executes the algorithm and produces and output with intermediate key-value pair. The Mapper output is passed to the Reducer as input. The Reducer executes the algorithm to create an output consisting of key-value pairs. The outcome of MapReduce is a summary where the values associated with each individual key are combined. \\

\includegraphics[width=8cm]{keyvalue} \\

In the simplest form of the MapReduce model, the user only specifies the Map tasks. In this case, the output will consist of the intermediate key-value pairs.

\section{The MapReduce Process}
%\label{sec:mapreduce}

The complete MapReduce process consists of four operations namely, split, map, shuffle \& sort and reduce.\\ The split and map operations are handled by the Map program, shuffle \&sort are handled by the Mapreduce framework and reduce is handled by the Reduce program.

1. Splitting - the process whereby larger data sets are split into smaller data sets based on the block size. The default size of each block is 64MB, however this can be adjusted by the user. The number of Mappers allocated will depend on the number of blocks generated by the split, i.e. if there are 3 blocks then there will only be 3 mappers.

2. Mapping - the purpose of the mapper is to process the input data. In this phase, data in each chunk is passed to a user specified map function which is executed and returns output in the form of key-value pairs. For example, if a file contains 100 blocks to be processed, you can have 100 mappers which can run together to process one block each or have 50 mappers running together to process two blocks each. The Hadoop framework decides how many mappers to allocate based on the number of nodes (CPUs), size of the data and the memory block available on each mapper server.\\

3. Shuffling \& Sort - the shuffle phase transfers the map output from the Mapper to the Reducer. The sort phase covers the merging and sorting of map outputs. The data from all Mappers are grouped by the key, split among the reducers and sorted by the key. The shuffle and sort phase occur simultaneously and are done by the MapReduce framework. This stage is necessary for the Reducers, otherwise there will be no input for them. Note that shuffle shuffle \& sort is not performed at all if zero reducers are specified in the job.

4. Reducing - the reducer is guaranteed to have all records associated with each key. In this phase, the key-value output from the shuffling stage are aggregated. It is important to note that one key can only be in a single reducer, else the aggregation will be non-functional. In other words, this operation is a summary of the complete dataset. 

\subsection{Example}

explain diagram

\includegraphics[width=14cm]{mapreduce} \\



\section{Advantages of MapReduce}

The main advantage of MapReduce is that it is highly scalable. This is due to the ability to process large data sets across multiple computer nodes. These servers can be inexpensive and can also operate in parallel. This simple scalability is what attracts many users to utilise the MapReduce model.

Other advantages include cost efficiency, security and authentication, parallel processing, availability, resilient and simple.


\chapter{The Hadoop Framework}

Hadoop is Apache's free and open-source implementation of the MapReduce framework. Apache Hadoop offers reliable, scalable, parallel and distributed computing scaling up from a single server to a network of multiple computers\cite{Ghazi15}. It was developed with the purpose of having a data store that allow organisations to leverage big data analytics with cost efficiency in mind.\\

%There are two main components of the Hadoop Framework: Storage and Processing

\section{Hadoop Architecture}

Hadoop follows a master/slave architecture design for data storage and distributed data processing using HDFS and MapReduce respectively. The master node for data storage is NameNode while the master node for parallel processing is the Job Tracker. The slave nodes are comprised of other machines in the Hadoop cluster which stores the data and performs the computations. Each slave node have a DataNode and a TaskTracker that synchronises the process respectively. \\

\begin{center}
 \includegraphics[width=10cm]{hadoop}\\
 \end{center}

The Hadoop system can be set up via cloud or locally. The cluster we will be running is a set up of 8 virtual machines.

\section{Storage - HDFS}

The storage component of the Hadoop architecture is known as the Hadoop Distributed File System (HDFS). The NameNode runs on the master node and manages metadata about the file system in a file named fsimage. This metadata is cached in main memory to provide faster access to the clients on read/write requests. The NameNode controls also manages the slaves by splitting files into chunks (default 64 megabytes) and distributing them across each DataNode in the cluster. The DataNodes are primary storage elements of HDFS where chunks of data are stored and replicated according to the instructions from the NameNode. Secondary NameNode is to periodically read the file system, log changes and applying them to the fsimage file. This will enable the NameNode to boot faster.\\

The main advantages of HDFS is data locality and fault tolerance. Data locality allow the nodes to manipulate the data they have access to which results in faster and more efficient processing while handling faults through the process of replicating files across each slave node[4]. \\

\includegraphics[width=8cm]{hdfs}

\section{Fault Tolerance in HDFS}

Fault tolerance refers to the ability for the system to handle unfavourable conditions, such as the failure of machine in the HDFS cluster. Fault is handled through the process of replica creation, which is to replicate the data on different machines in the HDFS cluster. So whenever a machine fail, the data can be accessed from other machines in which the same copies of data were created.

Suppose the user stores a file XYZ. HDFS will break this file into blocks, say A, B, and C. Let’s assume there are four DataNodes, say D1, D2, D3, and D4. HDFS creates replicas of each block and stores them on different nodes to achieve fault tolerance. For each original block, there will be two replicas stored on different nodes.

\includegraphics[width=8cm]{faulttol}

Let the block A be stored on DataNodes D1, D2, and D4, block B stored on DataNodes D2, D3, and D4, and block C stored on DataNodes D1, D2, and D3.

If DataNode D1 fails, the blocks A and C present in D1 are still available to the user from DataNodes (D2, D4 for A) and (D2, D3 for C).

Hence when one node breaks down, there won't be data lost. 

%insert diagram

\section{Processing - MapReduce}

In a MapReduce job, the input is broken down into multiple chunks which are processed by the map phase and then the output of the map phase is passed as input to the reduce phase. The input and output files are stored in the file system, while the output of the map phase (known as intermediate results) are only stored temporarily in the process. As the process runs in parallel, the reduce task on specific nodes may begin once its map tasks is complete rather than waiting on all map tasks to complete.\\

Similar to HDFS, the MapReduce process also utilises the master/slave architecture in which the JobTracker runs on the master node while the TaskTracker runs on each slave node. \\

\includegraphics[width=10cm]{jobtracker}\\

The JobTracker monitors the MapReduce tasks carried out by the TaskTracker running on the slave nodes. The user will only interact with the master node by submitting jobs to the JobTracker. The JobTracker then locates and submits the jobs to the TaskTracker. The slave nodes are monitored by the JobTracker through heartbeat signals to determine whether a node has failed. The JobTracker is a point of failure for the Hadoop MapReduce service and if it goes down, all jobs will be stopped. \\

The TaskTracker runs on the slave nodes in a cluster and receives jobs from the JobTracker to execute the MapReduce tasks. Each TaskTracker has an allocation of task slots which indicate the number of tasks it can accept. The JobTracker will delegate jobs to the TaskTracker based on the number of available/empty slots while the TaskTracker will periodically send heartbeat signals to inform the JobTracker of any issues. If one task dies either during the map or reduce phase the JobTracker is able to restart and repeat the task by assigning it to another node. Hence TaskTracker failure is not considered fatal as tasks can be reallocated when it becomes unresponsive.\\

In the case the JobTracker does not receive any heartbeat from a TaskTracker for a period of time (default is set to 10 minutes), the JobTracker will know that the worker has failed. The JobTracker will reschedule all pending tasks to another TaskTracker. All the completed map tasks need to be rescheduled if they belong to incomplete jobs because the intermediate results may not be accessible to the reduce task. 

\chapter{Streaming in Hadoop}

\section{What is Streaming}

\includegraphics[width=14cm]{streaming}\\

When we think of a stream we often picture water flowing through a fixed channel or pipe. Likewise in computer programming, it refers to the flow of data. A stream is basically a sequence of data bits made available over time from one node to another. From one node to the next, each channel involves depicted by the arrow is a pipe, which includes standard input and standard output. The program can get input from a source by reading in data as standard input. This input is the standard output of the source. Likewise, the standard output of the program is read into the destination as standard input. 

We refer to the complete process as a stream. In terms of Hadoop Streaming, the data is streamed from HDFS to another program and back to HDFS.

\subsubsection{Example}

We can look at a basic example to understand how streaming works. We have a comma-separated values (csv) file stored in HDFS that we will stream into R.

The first command creates a pipe with connects the csv file in HDFS with R. The file is read into the R using the read.table function and saved into an R data frame object called 'd'. Note that we specified that our file contained a header, and each field is separated by a comma and the number of rows we read in is 5. Lastly, we output the data using the print function. 

\section{Hadoop Streaming}

Hadoop streaming makes use of the streaming process which we discussed earlier. It is a utility that comes with the Hadoop distribution which allow users to create and run MapReduce jobs to be executed in the Hadoop cluster using other languages. By default the Hadoop MapReduce framework is written in Java but it utilizes Unix streams to pipe between Hadoop and our MapReduce program so we can use any language which can read standard input and write to standard output for our MapReduce program. MapReduce programs can be written in multiple languages such as R, Python, Perl, PHP, C++, etc. The utility is packed in a JAR file. 

Using the utility, we can create executable scripts to run MapReduce Jobs. Furthermore, we can create executable scripts to run the mapper and reducer functions which are passed to Hadoop streaming. The utility creates map and reduce jobs and submits them to the cluster which can then be monitored, thus enabling a person to write MapReduce job in the language of their choice without having any knowledge of Java.

\section{Syntax}

The syntax below can be used to run MapReduce jobs written in a different language to process data using the Hadoop MapReduce framework.\\

\begin{verbatim}
$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-streaming.jar 
    -input myInputDirs 
    -output myOutputDir 
    -mapper wc -l 
    -reducer awk '{n=n+$1}END{print n}' 
\end{verbatim}

The input command is used to provide the directory fo the input where the output command is used to provide the output directory. The mapper command is used to specify the executable mapper class while the reducer command is used to specify the executable reducer class. 

In the example above, myInputDirs is the input directory for the mapper and myOutputDir is the output directory for the reducer. The mapper executes the word count linux command on the input data which is fed to the reducer which aggregates result of the word count as a total and then returns the output to the output directory. 

\section{Implementation of Hadoop Streaming}

Let us now explore how Hadoop streaming works.\\

\includegraphics[width=14cm]{hadoopstreaming1}\\

In the above example image, we can see that the flow shown is a basic MapReduce job. Our main focus will be on the processes in the Mapper and Reducer stream.

\subsection{InputFormat}

We have an Input Reader which defines the input specifications for the MapReduce job. it is responsible for reading the input data producing the structure for the Map process. We can read data from any source which may be structured or unstructured. This includes csv formats, delimiter formats, from a database table, images, audios etc. The only requirement to read all these types of data is that we have to create a particular input format for that data with these input readers. The input reader contains the complete logic about the data it is reading. We want to be able to structure the data in a way that the mapper can interpret and process. Hence we have to specify the logic which defines the structure of the input.

\subsection{Mapper \& Reducer}

Mappers and Reducers are the Hadoop servers that run the Map and Reduce tasks respectively. The tasks involve reading in the data, processing the data through user defined scripts, and outputting it. Through Hadoop Streaming, the data is read into a different program where it can be processed using different language, and output back into the default Hadoop server.

When the mapper is initiated, the input is broken down into multiple blocks known as input splits defined by the Input Format. Each mapper task associated with each block will convert its input into lines to the standard input (stdin) of the process. The Mapper essentially runs the formatter function on the input data from the standard input to convert the file into a format that can be executed by the program. The map program then executes the mapper function that performs the calculation/ reduce job. Once the script is executed by the map program, the mapper collects the line-oriented outputs from the standard output (stdout) of the process and converts each line into structured key/value pairs known as the intermediate values. The intermediate values are structured in a way where the prefix of a line up to the first tab character is the key value by default and the rest of the line will be the value associated with the key. If there is no tab character in the line, then the entire line is considered as the key with no value associated. After all the mappers tasks are complete, the framework shuffles and sorts the results. The output of the mapper is fed to the reducer task. 

The reducer runs the same process as the mapper where the intermediate values are now parsed into the reduce program. The reducer runs the formatter function on the input key/value pairs followed by the execution of the reducer function that performs the calculation/ reduce job. Once the job is executed by the reduce program, the reducer converts its input key/value pairs into lines and converts into tab separated key-value pairs. The output of the reducer is the final output of the MapReduce job. 

\subsection{OutputFormat}

The OutputFormat determines where and how the the results of the job are persisted. It specifies how to seralize data by providing an implementation of RecordWriter. The RecordWriter handle the job of taking an individual key-value pair and writing it to the location prepared by the OutputFormat. There are two main functions of a RecordWriter which are 'write' and 'close'. The 'write' takes the output from the job and writes the bytes to disk, The default RecordWriter is LineRecorderWriter which writes the output as the key's bytes followed by a tab delimiter and the value's bytes followed by a new line. The 'close' function closes the Hadoop data stream to the output file. 

\section{Advantages}

Below are some of the advantages of using Hadoop Streaming.

\begin{enumerate}
\item Availability - the utility comes with the Hadoop distribution hence does not require further installation of softwares.
\item Learning - relatively easy to learn as it requires basic unix coding.
\item Reduce Development Time - it is much quicker to write mapper and reducer scripts/functions whereas using native Java MapReduce application is more complex as it requires the application to be complied, packaged, and exporting the JAR file.
\item Faster Conversion - it takes very little time to convert data from one format to another using Hadoop streaming especially when the input and output formats are specified.
\item Testing - input and output data can be tested quickly by using it with Unix or command line tools.
\end{enumerate}

\chapter{HMR Hadoop MapReduce Package for R}

We will be using R to stream and execute MapReduce jobs in the Hadoop cluster. \\

HMR is a package developed in R which acts as an interface which give users the ability to execute Hadoop MapReduce jobs directly through R. The MapReduce job can be completely submitted using R without the user having to access Hadoop or code in Java. The package enables Hadoop streaming as it allows the user to specify the Input Format, the Map and Reduce function, and the Output Format through R functions and R data structures such as matrices and data frames.

High efficiency is achieved through parsing data in binary and utilises chunk-wise processing as well as automated conversion to and from R objects. This enables a smooth and efficient transition between R and Hadoop. There are also other packages available in R which uses the traditional key-value operations such as the \pkg{rmr2} package.

\section{Examples}

Using the current package, we will perform some simple calculations using the taxi data set store in HDFS. We can access the files through Unix shell and simple Bash commands.

\begin{verbatim}
hadoop@hdp:~$ hadoop fs -ls taxi
\end{verbatim}

This data set contains 19 attributes ranging from vendor ID, pickup/drop off times, distance, location, payment, number of passengers, etc. \\

The following R script streams the MapReduce process through the HMR function in R. The script will calculates the number of lines/records for January 2015. with the map function being the shell command for counting the number of lines. The script uses the default formatter as no formatters have been specified.

\begin{verbatim}
hmr(hinput("taxi/2015/01"), map="wc -l", reducers=0, wait=FALSE)
\end{verbatim}

The following R script also performs the same line count calculation. However, the formatter has been specified which only include the first 5 fields of the data set. Instead of using shell command as the previous script, the mapper is using a R function "nrow" which also calculates the numbers of rows. The difference is that the number of reducers has been set to 1. This will ensure that the we only receive one output.
 
\begin{verbatim}
hmr(hinput("taxi/2015/01",
           formatter=function(o) dstrsplit(o, list(vendor=1, NA, NA, 
           	pass=1, dist=1), sep=",", strict=FALSE)),
    map=function(d) nrow(d), reducers=1, wait=FALSE)
\end{verbatim}

The output is saved to a directory in HDFS and can be access with the unix command hadoop fs -ls /folder name and printed using hadoop fs -cat /folder name. As we did not specify a reduce function, we return the output of the mapper which is the row count. Each block of data is assigned to a single mapper program and this case we have 15 mapper producing 15 blocks of output. \\ 

\begin{verbatim}
hadoop@hdp:~$ hadoop fs -cat /tmp/io-hmr-temp-30397-0831adbdb7fb1e3e/*
686375	
861469	
861493	
861503	
861529	
861574	
861610	
861626	
861649	
861652	
861678	
861681	
861691	
861715	
861742	
\end{verbatim}

\section{Concept of Formatting - Specifying Structure for Input Data}

When data is streamed from an external source into R, we need to define the structure of the data file for R to interpret. \\

Below is an example of parsing data from local disk to R. We use the R function read.table to illustrate how data is parsed. We added specifications on how the data is read into R. To ensure the data is read in correctly, we can specify the header, the separator symbol for each attribute, the data type for each attribute and the number of rows. 

\begin{verbatim}
> d <- read.table(file="data/taxi.csv", header=TRUE, sep=",", 
				colClasses=c("integer", "numeric", logical", rep("character", 6), nrows=5)
> d[,c(1,3,4,5)]
  VendorID tpep_dropoff_datetime passenger_count trip_distance
1        2   2015-01-15 19:23:42               1          1.59
2        1   2015-01-10 20:53:28               1          3.30
3        1   2015-01-10 20:43:41               1          1.80
4        1   2015-01-10 20:35:31               1           .50
5        1   2015-01-10 20:52:58               1          3.00
\end{verbatim}

The consequence of incorrectly parsing data into R will lead to issues when we try to compute on the data later on. 

\section{Formatter in HMR}

When we stream data from Hadoop, we are essentially creating a connection between HDFS and R. The above example works great when we can read in data from a structured file. What if our data file is not structured in a way we can easily read, or what if the file is too big for us to read efficiently. 

The solution is to write a R function that defines the structure of the input data we want to parse. We call this the Formatter. This is a lot more powerful than data input functions which requires the file to be structured. With the formatter function, we can parse in any type of data. In the HMR package, the formatter function, we provide the specifications, or in other words, the metadata which is the data that provides information about our data file. This includes information like the separator we use for each field, how many fields we have, the classes/ data types of our fields, etc.

\section{Hadoop Streaming in R}

We have discussed the idea of how Hadoop Streaming works, let us now see how we can stream the MapReduce process in R using the HMR package. The HMR package is essentially a Hadoop streaming API which runs in R. This allows the user to run MapReduce jobs and write formatter functions in R as opposed to writing scripts in the native language Java. \\

The first step is to specify the connection which is to define the HDFS path and input source. Once we have the connection established, the data can be parsed into R. The data is separated into blocks consisting of lines/rows referred to as chunks and the data is parsed as raw bytes which is much more efficient than parsing data as character values or any other data types. 

For R to process the MapReduce program, we will need to format each chunk of data to a structure that R can interpret. This is the next step to specify the formatter functions for the mapper and the reducer. In R, this involves writing function that will convert the raw bytes into a rectangular structure with rows of records and columns of attributes (fields). Within each column, we can also specify the class/data type associated with each field. The default classes in R include integer, numeric, logical, character, complex, POSIXct, etc. The structure of the data as a result of executing the formatter should return an R object which is typically in the form of a matrix or data frame.

The next step is to specify the mapper and reducer functions for the map and reduce process. The nice thing here is that our functions are no longer in Java, we can simply write these function using R code to process and compute on the R data object which we have just converted. Each chunk of data will be associated with each map process. The mapper takes in the R object consisting of values and executes the user defined mapper function. The mapper function identifies the keys from the values associated with it and generates the output R data frame with the key and value/s separated by a tab delimiter. The data frame generated by the mapper is converted back to raw bytes before it is parsed onto the reducer as input. The raw bytes now contain the structure of the mapper output which contain key value pairs separated by a tab delimiter. 

If the reduce function is not specified, the output will be parsed back to Hadoop HDFS. We will just receive the output of the mapper which is the shuffle and sorting of the key and its associated values without being reduced/aggregated. If we have specified the reduce function, the process follows the same process as the mapper. The reducer executes the formatter function and parse the data form the mapper output. Again, it is split into chunks, however this is dependent on how many reducers we have specified. The number of reducers will determine the number of chunks which determines the number of files generated output.

Each chunk of data will be associated with each reduce process. The reducer takes in the R object consisting of key-value pairs and executes the user defined reduce function. The objective is to take all the values that have the same key assigned to a single reducer which then aggregates/combines the values for that key. The output generated by the reducer is converted back to raw bytes before it is parsed back to Hadoop and stored in HDFS in a set directory. 

\section{Leveraging\pkg{iotools} for Efficiency}

The HMR package is highly efficient as it leverages the \pkg{iotools} package in \R. It is a set of tools for quickly importing and processing datasets using any data source. The functions packaged in \pkg{iotools} are comparatively faster than the native functions in R as well as other packages such as bigmemory, readr, foreach, etc. 

The two functions chunk.reader and read.chunk are functions used in the HMR package to read data as multiple lines as opposed to reading line by line sequentially. Chunk reader reads by default 32Mb (as default) of lines from a binary connection and stores it as an R object while preserving the integrity of the lines. Chunk.read then converts this object into a raw vector for each subsequent block of 32Mb. By parsing blocks of lines rather than single lines, we are able to achieve higher efficiency for data streaming.

Other core functions in the \pkg{iotools} package used in the HMR package are \code{mstrsplit} and \code{dstrsplit}. \code{mstrsplit} takes a raw or character vector and converts it into a character matrix according to the separators, while \code{dstrsplit} takes the vector and convert it into a dataframe instead. These two functions are primarily used to convert the raw output from the chunk functions into a R object which we will feed into our MapReduce process in R. These functions are able to minimize the copying of data and avoid the use of intermediate strings to improve performance.

The other function we will be leveraging is \code{as.output} which is the reverse of \code{mstrsplit} and \code{dstrsplit}. \code{as.output} essentially converts our R object back into the raw bytes while preserving the metadata/structure of our data. This function is used to output our data after we have executed our map and reduce programs. 

\section{Current Formatting Logic}

In order for us to execute MapReduce jobs, we require a server/cluster with Hadoop set up. The file path/connection defines the HDFS file path and input source. HMR can take two types of input objects: \code{hpath} and \code{hinput}. Hpath only contains the path of the input whereas hinput has an additional formatter argument which allows the user to specify the class of the input variables as a vector or a list. This will require the user to have some familiarity or knowledge of structure of the data. Once the connection is specified, input is read into R using chunk.reader and chunk.read from \pkg{iotools}. 

The current logic allows the user to specify the formatter for both the mapper and reducer as a list or the map or reduce formatter as a vector. If a single formatter function is specified, the same formatter will be used for both the map and reduce process. If no list or vector is specified for the formatter, then HMR defaults to the default formatter. The default formatter leverages the \code{mstrsplit} function that takes a raw vector which in our case is our data from HDFS and convert it into a matrix with a key and value structure. The key is the first column of the matrix separated from the values by a tab characters, however the values are returned as character types which is undesirable. The downside of this is that "character" data type requires more storage compared to other data types such as logical and integers. Having the incorrect data type also means we will need to convert our values to the correct type before we can compute on it. This can be resolved by converting the data type in our map and reduce functions.

\section{Example of Formatter Function}

\begin{verbatim}
hinput("taxi/2015/01",
           formatter=function(o) dstrsplit(o, list(vendor=1,NA,NA,pass=1,dist=1), 
           sep=",", 
           strict=FALSE)
\end{verbatim}

\section{Motivation for Enhancements}

So far we have looked at excuting the MapReduce program with only the Mapper. To infer the input data for the Mapper is relatively simple as we have visibility of the data in HDFS which we can easily infer the class/data type, however, this is not as simple for the Reducer.  As the output of the Mapper is parsed into the Reducer, we have no direct access to the Mapper output throughout the complete MapReduce job. This means we cannot infer the input data for the Reducer. If we attempt to write the formatter function for the Reducer, we may have to resort to trial and error to guess the input to the Reducer. An alternative approach is to first execute the job with only the Mapper process then use that output to infer the data for the Reducer. This method is inefficient, specifically when the purpose is to process and analyse large data sets. Our motivation is execute the MapReduce program without having to run multiple processes or resort to testing or guessing the Reducer formatter function.

\chapter{Package Enhancement - Automation of Formatters}

The current specifications in HMR allows the user to specify the formatter for the Mapper and the Reducer or the package defaults to the default formatter which is highly inefficient due to the variables being parsed as strings/characters. This may lead to running additional MapReduce jobs in order to reach the final desired output. It also adds more complex to the reducer function in the case where the data type may needs to be converted appropriately for the reducer to run successfully. 

We know the importance of formatters with regards to writing the map and the reduce functions to execute the MapReduce job. Our goal is to be able to efficiently execute the MapReduce program in R, however the current approach suggest that we have difficulty in the processing of the Reduce task in terms of formatting the input for the Reducer. The idea is for us to find a way to automatically detect the formatter function, and specially for the Reduce task. This feature is added to the\pkg{HMR} to increase efficiency and versatility for users of this package.

We will look at how we can automate the formatter function for the Mapper and the Reducer using different approaches and at the same time provide customisations for the user to select between different formatting approaches.

\section{Concept around Auto Detection}

We explored how to specify the formatter functions in\pkg{HMR} and we know drawbacks when the functions are not specified for the Mapper and Reducer. We will still provide the output with the cost of reduced efficiency as a result of using the \texttt{.default.formatter}. How can we simplify the steps and streamline the\pkg{HMR} package. In other words, how can we efficiently execute the MapReduce program in R without the need to manually specify the formatters. 

This is achieved by using algorithm to inferring the data type from the input data to the Mapper and the Reducer. The idea is that we take a sample of the input data and use that to infer the data type for the values/attributes and assign the data types as column types to the formatter function. The formatter function is assigned to the Map/Reduce tasks for converting the data into a usable format. There are two approaches that we can take to infer the data type to automatically detect the formatter function for the Mapper and the Reducer.

\begin{enumerate}
\item Dynamic approach - generates and executes the formatter function during the Map and Reduce process (in between). Each process will be assigned a unique formatter function.
\item Static approach - generates and executes a single formatter function for each Map and Reduce process prior to executing the complete MapReduce job (ahead of time). The single function will be applied to all Mapper and Reducers.
\end{enumerate}
 
Due to the nature of MapReduce, the Mapper do not have a way to communicate directly with the Reducer. Hence, we motivate the different approaches to detect ahead of time. The two approaches will have their advantages and drawbacks dependent on type of data that we are processing. We will discuss these towards the end of the article.

\subsection{Auto Formatting - Mapper}

As we mentioned in the chapter above, inferring the data type for the mapper is relatively easy as we have access to the input data from HDFS. The idea here is to automatically generate the formatter for the Map process. We will explore idea behind the two formatting approaches for the mapper.

\subsubsection{Dynamic Approach}

The dynamic approach formats the input data during the Mapper stage which is streamed through R. The input data for the Mapper is structured in a way to only contain values. The dynamic approach reads a subset of the raw input data as a matrix and uses it to infer the data type for each column. The data types for each value/attribute are character values which are stored in a vector. This vector containing the column types is passed to the formatter function which is assigned to the map formatter function responsible for formatting the input data on each chunk. The formatter function is a R function which defines the structure of the input data we parse to the Map process. In the MapReduce job we will have a vector of column types for each Mapper which generates a distinct formatter function for each data chunk. The output of the mapper is a key value structure which is converted to raw bytes.

\subsubsection{Static Approach}

The static approach formats the input data ahead of time, in other words, before the MapReduce job begins to execute. The static approach first creates a connection to the HDFS directory and reads a sample from the raw input data. The data type is inferred from this subset which also generate a vector containing the column types for each value/attribute. The difference is that the subset is only inferred once as we will rely on this single vector containing the data types/class to represent the format of the whole data set.
Hence, this approach generates a single formatter function which contains the metadata of our input data to be executed across all the mappers in the job. The output of the mapper is a key value structure which is converted to raw bytes.

\subsection{Auto Formatting - Reducer}

To infer the data type for the Reducer is quite difficult as we do not have immediate access to our input data which is also the output of the Mapper. Unlike the input data to Mapper, the input to the Reduce is not directly accessible on disk ahead of time. The only way that we can access the Reducer input is to run the MapReduce job with the Mapper process. The idea here is to automatically generate the formatter for the Reduce process without needing to run multiple processes. We will explore idea behind the two formatting approaches for the reducer.

\subsubsection{Dynamic Approach}

The dynamic approach formats the input data during the Reduce stage. The dynamic approach for the Reducer is similar to the mapper as it reads the data during the executing of the Reduce phase. The difference for the Reducer is that the input data now contains a key-value structure which the formatter function will need to define. The Reducer reads the raw input data from the Mapper output as a matrix and uses it to infer the data types for each column. The inferred data types are stored in a vector containing the column types for each value/attribute. This vector containing the column types is passed to the formatter function assigned to the reduce formatter allocated for each Reducer. In our MapReduce job we will have a vector of column types for each Reducer which executes a separate formatter function containing the metadata of our input data depending on the number of Reducers specified. The output of the Reducer retains the key value structure which is converted to raw bytes.

\subsubsection{Static Approach}

The static approach for the Reducer formats both the input data of the Map phase and the Reduce phase prior to executing the MapReduce job. What we are essentially doing is running an extra Map process on the sample of the complete data to infer the data types of the Mapper output which we can then use to create the formatter function for the Reducer. In this approach, a connection is created to the HDFS directory to parse a sample of the raw input data. The data type is again inferred from this subset to produce a vector containing the column types for each value/attribute. At this point the vector of column types are only associated with the input for the Mapper, hence the Mapper process is executed on the subset to produce the intermediate key value pairs as output. From the output of Mapper, we can read the input and infer the data types to create the vector of data types for the values/attributes. This vector is assigned to the formatter function for the Reducer. This way the static approach creates a single formatter function for both the Map phase and the Reduce phase. The output of the Reducer retains the key value structure which is converted to raw bytes.

\section{Implementation in HMR}

%talk about the idea of running in dynamic and in advance

The enhancement provides automatic detection of the formatter for the Mapper and the Reducer in the absence of user specified formatters. The additional argument included in the HMR function is "autoformatter" which is set to TRUE for dynamic detection, FALSE for static approach and defaults to NULL if the user choose to specify their own formatters.

There are two approaches for detecting the formatter. The original approach is to detect the formatter in advance which we refer to as the "static" approach. The alternative approach is to run the on each chunk during the MapReduce process. This is referred to the "dynamic" approach.

\subsection{Taxi Data Set}

Below is the data set used to provide examples and illustrate how the two approach work in R. The data set is already subsetted containing 25 records with the first 6 fields.

<<echo=FALSE, include=FALSE>>=
    library(iotools)
library(knitr)
    options(prompt = "> ", continue="  ", useFancyQuotes = FALSE)
opts_chunk$set(comment = NA,
               prompt = TRUE,
               fig.align = 'center',
               fig.show = 'hold',
               tidy = FALSE,
               size = 'footnotesize',
               cache = FALSE,
               cache.path = 'MyKnitrFigs/mycache/',
               highlight = TRUE,
               continue = "  ")  # Usually a "+"


r <- charToRaw("VendorID,tpep_pickup_datetime,tpep_dropoff_datetime,passenger_count,trip_distance,
2,2015-01-15 19:05:39,2015-01-15 19:23:42,1,1.59,
1,2015-01-10 20:33:38,2015-01-10 20:53:28,1,3.30,
1,2015-01-10 20:33:38,2015-01-10 20:43:41,1,1.80,
1,2015-01-10 20:33:39,2015-01-10 20:35:31,1,.50,
1,2015-01-10 20:33:39,2015-01-10 20:52:58,1,3.00,
1,2015-01-10 20:33:39,2015-01-10 20:53:52,1,9.00,
1,2015-01-10 20:33:39,2015-01-10 20:58:31,1,2.20,
1,2015-01-10 20:33:39,2015-01-10 20:42:20,3,.80,
1,2015-01-10 20:33:39,2015-01-10 21:11:35,3,18.20,
1,2015-01-10 20:33:40,2015-01-10 20:40:44,2,.90,
1,2015-01-10 20:33:40,2015-01-10 20:41:39,1,.90,
1,2015-01-10 20:33:41,2015-01-10 20:43:26,1,1.10,
1,2015-01-10 20:33:41,2015-01-10 20:35:23,1,.30,
1,2015-01-10 20:33:41,2015-01-10 21:03:04,1,3.10,
1,2015-01-10 20:33:41,2015-01-10 20:39:23,1,1.10,
2,2015-01-15 19:05:39,2015-01-15 19:32:00,1,2.38,
2,2015-01-15 19:05:40,2015-01-15 19:21:00,5,2.83,
2,2015-01-15 19:05:40,2015-01-15 19:28:18,5,8.33,
2,2015-01-15 19:05:41,2015-01-15 19:20:36,1,2.37,
2,2015-01-15 19:05:41,2015-01-15 19:20:22,2,7.13,
2,2015-01-15 19:05:41,2015-01-15 19:31:00,1,3.60,
2,2015-01-15 19:05:41,2015-01-15 19:10:22,1,.89,
2,2015-01-15 19:05:41,2015-01-15 19:10:55,1,.96,
2,2015-01-15 19:05:41,2015-01-15 19:12:36,2,1.25")

subset = mstrsplit(r, sep=",", nsep="\t", nrows=3, skip=TRUE)

coltypes <- function(r, sep=",", nsep='\t',
                       nrowsClasses=25L, chunksize=size, header=TRUE) {
      subset = mstrsplit(r, sep=sep, nsep=nsep, nrows=nrowsClasses, skip=header)
      apply(subset, 2, function(x) class(type.convert(x, as.is=TRUE)))
  }

<<>>=
cat(rawToChar(r))
@ 


\subsection{Dynamic Approach}

With the dynamic approach, the goal is to read input data stored in raw bytes to a matrix object in R and infer the data type for each column. We create a function in R that is able to process this. We call this function \texttt{coltypes}. 

\begin{verbatim}
 coltypes <- function(r, sep=formsep, nsep='\t', 
 	nrowsClasses=25L, header=TRUE) 
 \end{verbatim}

The function takes the following inputs:

\begin{itemize}
\item \textbf{r} - input in the form of a raw vector 
\item \textbf{sep} - separator character
\item \textbf{nsep} - key separator character
\item \textbf{nrows} - number of rows
\item \textbf{header} - whether the input data contains a header as the first line
\end{itemize}

The first step of \texttt{coltypes} is to create a matrix from the vector of raw input. We use the function \texttt{mstrsplit} from \pkg{iotools} to read in the data using the specified inputs to define the structure of the matrix. It is important to note that each field is a character object as we did not specify the data type for each field.

\begin{verbatim}
subset = mstrsplit(r, sep=sep, nsep=nsep, 
    			nrows=nrowsClasses, skip=header)
\end{verbatim}

The illustration below shows the matrix that is returned from the raw input using \texttt{mstrsplit}. We specify that we want the separator to be comma, tab as key separator, 5 rows and skip the header. We want to always skip the header so it does not appear in our matrix used to infer the data types.

<<>>=
sep=","; nsep='\t'; nrowsClasses=5L; header=TRUE
mstrsplit(r, sep=sep, nsep=nsep, nrows=nrowsClasses, skip=header)
@ 

The next step is to infer the data types for each column of the matrix which represents the attributes of our data set. Based on the fields in each column, we convert the data type to the appropriate class. This is achieved using the \texttt{type.convert} function which converts the character object to a logical, integer, numeric, complex or character based on all the rows in the field. The \texttt{class} function will return the data type of each field and using \texttt{apply} we return the data type for all the rows in the columns.

\begin{verbatim}
colClasses = apply(subset, 2, function(x) 
    				class(type.convert(x, as.is=TRUE)))	
\end{verbatim}

The illustration below shows the vector that is created from inferring the data types for each column of the matrix. The output of of the \texttt{coltypes} function is simply a vector that contains the data type of each field stored in a vector. This output is assigned to the column type in the formatter function assigned to the process (Map or Reduce) it is running. Each individual process will execute the \texttt{coltypes} function and generate its own formatter function for each chunk of data.

<<>>=
apply(subset, 2, function(x) class(type.convert(x, as.is=TRUE)))	
@ 

Each formatter function is defined by a \texttt{dstrsplit} function from \pkg{iotools}. The \texttt{dstrsplit} function is ultimately used to convert the full chunk of raw input to a R data frame given the structure of the input. The column\_types input for the formatter function is the \texttt{coltypes} function which infers and provides the column type.

The specifications in the formatter function for the Mapper requires same separator used to infer the data as this is the separator for the input data. The header is also removed as we do not want to read the header as the first line.

\begin{verbatim}
map.formatter <- function(x) dstrsplit(x, col_types=coltypes(x), 
          sep=sep, skip=TRUE)
\end{verbatim}

The specifications in the formatter function for the Reducer requires the key value pair structure. Hence, we require the key separator to be a tab character. There will be no header in the input to the Reducer hence we do not need to skip the header for inferring the data type as well as the formatter.

\begin{verbatim}
red.formatter <- function(x) dstrsplit(x, 
          col_types=coltypes(x, header=FALSE), nsep='\t')
\end{verbatim}

\subsection{Static Approach}

With the static approach, the goal is to read input data, infer the data type and return a single formatter function for each Mapper and Reducer. We also create a function in R capable of performing this. We call this function \texttt{guess}. The \texttt{guess} function is executed ahead of time, in other words prior to the executing of the MapReduce job.

\begin{verbatim}
guess <- function(path, chunksize, header=TRUE, map) {
\end{verbatim}

The function takes the following inputs:

\begin{itemize}
\item \textbf{path} - the path of the connection (to HDFS)
\item \textbf{chunksize} - size of the raw input for subsetting
\item \textbf{header} - whether the input data contains a header as the first line
\item \textbf{map} - user defined map function 
\item \textbf{map.formatter} - this is set to attr(input, "formatter") which checks if the user has specified the formatter for the mapper
\end{itemize}

The first step in \texttt{guess} is to create the connection and read the input into R. The connection between HDFS and R is created using the \texttt{pipe} function. 

\begin{verbatim}
f <- pipe(paste("hadoop fs -cat", shQuote(path)), "rb")
\end{verbatim}

Then we use \texttt{chunk.reader} and \texttt{read.chunk} to read in the data from HDFS as a raw vector. Instead of taking the first 25 rows of each chunk, the static approach takes by default 1 million bytes of input data from the top and use that to infer the column types. As the chunk operations have the ability to preserve the row records hence we do not need to worry about where the last row ends.

\begin{verbatim}
cr <- chunk.reader(f)
r <- read.chunk(cr, chunksize)
\end{verbatim}

Then we execute the \texttt{coltypes} function on the subset raw vector to output the vector containing the data types for each column.

\begin{verbatim}
colClasses = coltypes(r)
\end{verbatim}

The static approaches leverages the \texttt{coltypes} function to output the vector ahead of time. This way we only generate one output which is executed across each chunk of data associated with each process. 

<<>>=
coltypes(r)
@ 

The static approach proceeds to check if the map function is provided. The map function is only provided as input to \texttt{guess} when the Reducer has been specified in HMR. The map process is run on the subset data using the Map formatter. The output are the intermediate key-values which is store in a data frame. 

\begin{verbatim}
m = map(dstrsplit(r, col_type=colClasses, sep=sep, skip=header))
\end{verbatim}

In order to execute the \texttt{coltypes} function on the Mapper output, it needs to be converted back to raw bytes which is done using \texttt{as.output}. This output is used to infer the column types for the Reduce formatter. A sanity check is applied to the column types vector to check that the length of the vector matches the columns of the Mapper output. The headers of the Mapper output are preserved by assigning the names of the Mapper output to the inferred vector of column types. This feature allows the user to specify the names of the values in the reducer function.

\begin{verbatim}
c = coltypes(as.output(m), header=FALSE)
if (length(c) == length(names(m)))
         names(c) = names(m)
\end{verbatim}

Given that the Reducer is specified in the job, the \texttt{guess} function will output the formatter function for both the Mapper and the Reducer. The column types for the Mapper formatter will be taken from colClasses which was inferred on the Mapper input, whereas the the column types for the Reduce formatter will be taken from the vector c which was inferred on the Mapper output/Reducer input. In the Map formatter, we use the separator from the input data in HDFS whereas in the Reduce formatter, the key is separated from the values with a tab and the values are separated with a vertical bar. If the Reducer is not specified in the job, only the Map formatter will be returned.

\begin{verbatim}
 if (!missing(map)) {
       if (is.null(map.formatter)) 
           list(map=function(x) dstrsplit(x, col_type=colClasses, 
                            sep=",", skip=header), 
           reduce=function(x) dstrsplit(x, col_type=c, sep="|", 
                            nsep="\t", skip=FALSE))
} 
else function(x) dstrsplit(x, colClasses, sep=formsep, skip=header)
\end{verbatim}

If the formatter in \texttt{hinput} is not null which suggest that the Map formatter is specified by the user, then the \texttt{guess} function will return the user specified formatter for the Mapper and the static formatter for the Reducer.

\begin{verbatim}
list(map=attr(input, "formatter")
      reduce=function(x) dstrsplit(x, col_type=c, sep="|", 
                            nsep="\t", skip=FALSE)
\end{verbatim}

The \texttt{guess} function returns a single formatter function for the Map and Reduce process which is applied to each of the data chunks accordingly.   \\

The complete functions for the dynamic and static approach can be found in the HMR source code.

\section{Formatter Logic}

Let us now examine the logic for both the Mapper and the Reducer. The following diagram applies to the Mapper and the Reducer separately.\\

\begin{center}
\includegraphics[width=14.5cm]{formatterlogic}\\
\end{center}

There are three conditions in the formatter logic that determines which formatter will be executed. The first is whether a user specified function is provided. If the user provides the formatter function for the Mapper and the Reducer, then this will take precedence. The second condition determines if the auto formatters are utilised. If the auto.formatter argument is not set in HMR, then the formatter function for the Mapper and the Reducer will be set to the default formatter which reads all fields as character values. The last condition involves the decision between executing the dynamic approach or the static approach. By setting the auto.formatter to TRUE the dynamic formatter is executed, and FALSE for the static formatters.

\subsection{Dynamic Formatter}

When the dynamic approach is executed in HMR, we expect two auto formatter functions for the Mapper and the Reducer given that the Reduce job has been specified. The dynamic formatters are predefined code which takes the inferred vector of column types as the input argument. There is the case when the user wish to specify the Map formatter manually and auto detect the Reduce formatter. In that case, the map.formatter takes the user input formatter. \\

The Map formatter is specified using the code below. \texttt{formsep} is an arugment into HMR which the user specifies as the separator the input data structure. 

\begin{verbatim}
if (inherits(input, "hinput")) {
       map.formatter <- attr(input, "formatter")
}
else map.formatter <- function(x) dstrsplit(x, coltypes(x), 
        sep=formsep, skip=TRUE)
\end{verbatim}

If the Reduce process is specified, the Reduce formatter will take the following code. As we have a key-value structure, the key separator \texttt{nsep} is set to tab character.

\begin{verbatim}
if (!missing(reduce))
      red.formatter <- function(x) dstrsplit(x, 
          coltypes=coltypes(x, header=FALSE), nsep='\t')
\end{verbatim}

\subsection{Static Formatter}

When the dynamic approach is executed in HMR, we expect two auto formatter functions for the Mapper and the Reducer given that the Reduce job has been specified. The static formatters are predefined in the logic which executes the \texttt{guess} function which executes the mapper process on a subset and returns the corresponding formatter functions. \\

If the reduce process is not specified, the static approach will produce the formatter function for the Mapper only. It takes the code below which executes the \texttt{guess} function without the map process. This returns a single formatter for the Map process.

\begin{verbatim}
map.formatter <- guess(paste0(input,"/*"))
\end{verbatim}

If the Reduce process is specified, both the Reduce and Map formatter will be generated from the following code. The \texttt{guess} function is executed with the map input which output a list of two formatter functions for both the Mapper and the Reducer. The formatter function for the Mapper is allocated to the map formatter and likewise for the Reducer. There is also the case when the user wish to specify the Map formatter manually and auto detect the Reduce formatter. In that case, the map.formatter takes the user input formatter. This logic is taken care of in the guess function discussed above.

\begin{verbatim}
        if (!missing(reduce)) {
          formatter <- guess(paste0(input,"/*"), map=map)
          map.formatter <- formatter$map
          red.formatter <- formatter$reduce
        }
\end{verbatim}


\section{Examples}

We will look at some examples of MapReduce jobs using autoformatters. 

The first example below performs the dynamic as autoformatter is set to TRUE. The map function assigns the rows of the data chunk to a user created key "A" and the reduce function sums the the values associated with the only key "A". One reducer is specify which generates one output file.

\begin{verbatim}
hmr(hinput("taxi/2015/01"), #autoformatter=TRUE,
    map=function(d) c(A=nrow(d)),
    reduce=function(o) sum(as.numeric(o)),
    reducers=1, wait=FALSE)
\end{verbatim}

The following results are obtained.

The second example 

\begin{verbatim}
## new case
# what if I want the total passenger for each vendor
hmr(hpath("taxi/2015/01"), autoformatter=FALSE,
    map=function(d) t(simplify2array(tapply(o[[4]], o[[1]], function(x)  c(sum(x), length(x))))),
    reduce=function(o) sapply(split(d, d$rowindex, function(o) sum(o[,2]) / sum(o[,3]))),
    reducers=1, wait=FALSE)
\end{verbatim}


\begin{verbatim}
hmr(hpath("taxi/2015/01"), autoformatter=FALSE,
    map=function(d) c(d[[1]], d[[4]]),
    #reduce=function(o) tapply(o[[2]], o[[1]], function(x) round(mean(x), 2),
                              reducers=1, wait=FALSE)
\end{verbatim}

\chapter{Discussions and Future Directions}

In this paper we discussed how we can define the structure of the input data and use that to infer the data type/class automatically before the data is processed by Mapper and Reducer programs. This chapter is to discuss the methods and possible future directions to take.

\section{Discussion around Auto Formatters}

One may be wondering what is the purpose of having two auto formatting methods when it provides the same results. In the sections below, we discuss some advantages and drawbacks around the two approaches and when one approach will be superior to the other.

\subsection{Preserving headers} 

The execution of the Mapper and Reducer results in an ouput with a structure that does not contain any headers. In the input file, header usually only appear once as the first row. When executing the dynamic approach, the header will only be applied to the first chunk, unless we have have multiple headers in our data file allocated for each chunk. This becomes tedious and adds complexity. The remedy for executing the dynamic approach will be to specify the column number in the mapper and reducer functions as opposed to specifying the header name.

% does as.output remove the header?

The issue with the header is resolved in the static approach. This is due the fact that we extract a subset of data ahead of time. After the mapper process is run on the subset data, the column names are assigned on the output which is used to infer and produce the reduce formatter. Hence the column names/headers can be utilised in writing the mapper and reducer function. 

\subsection{Empty subsets} 

If the subset data from static approach turns out to be empty or contains descriptions rather than input data, we will not be able to infer the data type and hence will result in empty output. For example, NA is interpreted as logical. If the fields in the subset all contain NA, then we will expect a logical vector. Because we base the structure of the complete data file on the subset, we need to ensure that the subset does not contain any missing or corrupted input. 

The issue around empty subset is resolved in the dynamic approach as each chunk will generate a unique formatter. Although we expect that each individual chunk holds the same structure and data type, in the case where there may be differences including missing data, we can still rely on the dynamic approach to run successfully and generate an output. 

\subsection{Efficiency of a single vs multiple formatters}

Do a benchmark test here?


\section{Further Directions}

We have focused on the formatters which reads data as the standard input from one source to another. In our case, we specifically focus on the input formatter of the mapper and the reduce in R. For future directions, we can also look at different methods of structuring data for the output of our mapper and reducer. 

Currently we leverage the as.output from the \pkg{iotools} package which preserves the R object as a raw vector, however the headers do not get preserved. We can look at outputting the results of the mapper and reducer with a different formats that encode meta information in each record. By doing so, we may be able to resolve some of the limitations listed above. For example, the column names and other meta information could be preserved using JSON by including the column names for each record row.

\bibliographystyle{abbrv}
\renewcommand{\bibname}{References} % changes the header; default: Bibliography
\addcontentsline{toc}{chapter}{References}
\bibliography{Project.bib}

\end{document}
